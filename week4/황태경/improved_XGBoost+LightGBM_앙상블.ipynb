{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df89bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 개선 3. LightGBM과 XGBoost 앙상블\n",
    "# LightGBM + XGBoost 두 모델의 예측값을 결합하기\n",
    "# 앙상블 : 여러 모델에서 얻은 예측 결과를 결합해 더 좋은 예측값을 도출하는 방식\n",
    "# c.f. 앙상블은 다양한 모델을 사용하는 게 바람직함.\n",
    "# 각 모델의 예측값이 거의 같다면 앙상블해도 큰 효과가 없음.\n",
    "# e.g. 타겟값 0을 잘 예측하는 모델과 타겟값 1을 잘 예측하는 모델을 앙상블하면 시너지 효과가 발휘함 ~ 성능 최고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d00f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능개선1. 베이스라인 모델을 그대로 사용하되, 피처 엔지니어링과 하이퍼파라미터 최적화 적용\n",
    "# + 파생 피처 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52328e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 모델링 전략\n",
    "# 고급 모델링 기법 : OOF 예측, 베이지안 최적화, LightGBM, XGBoost 앙상블\n",
    "\n",
    "# 1. BaseLine : LightGBM, 마이크로소프트 ~ XGBoost와 함계 캐글에서 가장 많이 사용하는 머신러닝 모델\n",
    "# 훈련과 예측 과정이 동시에 이루어진다.\n",
    "\n",
    "# 데이터 불러오기\n",
    "data_path = 'data/'\n",
    "\n",
    "# train\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col = 'id')\n",
    "\n",
    "# test\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col = 'id')\n",
    "\n",
    "# sample 결과\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070594ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 엔지니어링\n",
    "# 머신 러닝 알고리즘의 성능을 향상하기 위해 데이터를 변환하고 개선하는 프로세스\n",
    "\n",
    "# 데이터 합치기 : 두 데이터를 동일한 인코딩을 적용하기 위함.\n",
    "all_data = pd.concat([train, test], ignore_index = True) # 기존 인덱스를 무시하며 train, test 합침.\n",
    "all_data = all_data.drop('target', axis = 1) # 타겟값 제거, 레이블 축 : 세로 이므로 1.\n",
    "\n",
    "# 피처만 떼어놓기\n",
    "all_features = all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54baf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처 원-핫 인코딩\n",
    "# 고윳값별 순서가 없기 때문에 적용가능.\n",
    "# 복잡한 데이터를 그대로 사용하지 않고 컴퓨터가 처리하기 쉽게 숫자로 변형해 줌.\n",
    "# 범주형 데이터가 순서나 크기의 의미를 포함하고 있을 때 원핫 인코딩을 하게되면 \n",
    "# 이러한 정보들은 사용할 수 없게된다는 것입니다. \n",
    "# 예를 들면 월요일, 화요일, 수요일, 목요일, 금요일, 토요일, 일요일을 원핫 인코딩하면 \n",
    "# 그저 0과 1일 집합으로 표현됩니다. \n",
    "# e.g. 월요일은 [1,0,0,0,0,0,0], 토요일은 [0,0,0,0,0,1,0], 일요일은 [0,0,0,0,0,0,1] 이렇게 되겠지요. \n",
    "# 이렇게 인코딩된 값들은 요일간에 순서나 크기를 표현하지 못합니다.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature] # cat이 포함된 feature만 뽑아냄.\n",
    "\n",
    "onehot_encoder = OneHotEncoder() # 원-핫 인코더 객체 생성\n",
    "\n",
    "# 인코딩\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a397fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파생 피처 추가 - 성능 향상을 위한 방법들\n",
    "# 1. 한 데이터가 가진 결측값을 파생 피처로 만든다.\n",
    "\n",
    "# '데이터 하나당 결측값 개수'를 파생 피처로 추가\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1) # y축으로 데이터 추가하므로 axis = 1\n",
    "\n",
    "# 결측값 개수를 num_missing이라는 피처명으로 all_data에 추가함. -> 각 데이터 피쳐벌로 결측값을 전부 더한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db73500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처, calc 분류의 피처를 제외한 피처\n",
    "\n",
    "# 1. 원핫 인코딩 적용한 피처 : 명목형 피처\n",
    "remaining_features = [feature for feature in all_features\n",
    "                     if('cat' not in feature and # 명목형 피처\n",
    "                        'calc' not in feature)]\n",
    "\n",
    "# num_missing을 remaining_features에 추가 - 파생 피처 추가\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ind 분류의 피처를 연결해서 새로운 파생 피처로 추가 : mix_ind\n",
    "# 분류가 ind인 피처\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature] # ind인 모든 피처를 가져온다.\n",
    "\n",
    "is_first_feature = True # 첫번째인지 판별하기 위한 flag\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_' # feature의 데이터 값을 str로 형변환 + '_' : 선언\n",
    "        is_first_feature = False # flag 제거\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) +'_'\n",
    "        \n",
    "\n",
    "# ind 분류의 모든 피처 값을 연결한 파생 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51081f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1     1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2    5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3     0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4     0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "Name: mix_ind, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148dbcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 명목형 피처의 고윳값 개수를 새로운 피처로 추가한다.\n",
    "# 피처의 고유값 각각의 개수는 value_counts()로 구할 수 있다.\n",
    "\n",
    "all_data['ps_ind_02_cat'].value_counts().to_dict()\n",
    "\n",
    "# 해당 피처 내에서, 고윳값 1, 고윳값 2 등등의 개수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8645949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처의 고윳값에 대한 파생 피처\n",
    "cat_count_features = []\n",
    "\n",
    "for feature in cat_features + ['mix_ind']: # c.f. mix_ind 피처도 명목형 피처이므로, 같이 추가해준다.\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict() #각 고윳값을 key로 갖는 dict\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x : val_counts_dict[x])\n",
    "    # all_data['@@_count'] = all_data[feature]의 모든 데이터에 고유값을 고유값의 개수로 치환한다.\n",
    "    # apply : mapping 함수.\n",
    "    \n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e36f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    4\n",
       "3    1\n",
       "4    2\n",
       "Name: ps_ind_02_cat, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e056af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     309747\n",
       "1    1079327\n",
       "2      28259\n",
       "3    1079327\n",
       "4     309747\n",
       "Name: ps_ind_02_cat_count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 고유값이 모두 고유값의 개수로 바뀐 것을 알 수 있다.\n",
    "all_data['ps_ind_02_cat_count'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66b0cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리\n",
    "# 1. encoded_cat_matrix : one-hat encoded 명목형 피처\n",
    "# 2. remaining_features : 명목형 피처와 calc 분류의 피처를 제외한 피처들 (+ num_missing 파생 피처)\n",
    "# 3. cat_count_features : mix_ind를 포함한 명목형 피처의 고윳값별 개수 파생 피처\n",
    "\n",
    "# 필요없는 피처 제거\n",
    "# 1. 이전에 분류한 피처들 \n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
    "                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "# reamining_features와 cat_count_features를 합치고, drop_features를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "324db2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 합치기\n",
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix],\n",
    "                              format='csr')\n",
    "\n",
    "# all_data_remaining과 encoded_cat_matrix를 합침.\n",
    "# hstack : 위 아래로 쌓아줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a152dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지 한 것 정리\n",
    "# 1. 명목형 피처에 원 핫 인코딩을 적용했습니다.\n",
    "# 2. 데이터 하나당 가지고 있는 결측값 개수를 새로운 피처로 만들었습니다. - encoded_cat_matrix\n",
    "# 3. 모든 ind 피처 값을 연결해서 새로운 명목형 피처를 만들었습니다. - mix_ind\n",
    "# 4. 명목형 피처의 고윳값별 개수를 새로운 피처로 만들었습니다. - cat_count_features\n",
    "# 5. 필요 없는 피처를 제거했습니다. (calc 분류의 피처, drop_features 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30d66634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 나누기 - 이전과 동일함.\n",
    "\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "# 전체 데이터를 훈련 데이터와 테스트 데이터로 다시 나눔.\n",
    "num_train = len(train) #훈련 데이터 개수 -> 훈련 데이터 개수 만큼 나누기 위함.\n",
    "\n",
    "# 셔플로 섞은 후, 나눠줌. -> hstack\n",
    "#all_data_sprs = shuffle(all_data_sprs)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57d49b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 작성 : 정규화된 지니계수\n",
    "# 지니계수 : 지니계수가 작을수록 소득수준이 평등하고, 클수록 불평등함. -> 로렌츠 곡선 이용하여 계산함.\n",
    "# 로렌츠 곡선 : 모든 경제인구를 소득 순서대로 나열한 후에 가로축은 인구 누적 비율, 새로축은 소득 누적 점유울로 설정함\n",
    "\n",
    "# 정규화 지니계수 : 정규화 후 0에 가까울 수록 성능이 나쁘고, 1에 가까울수록 성능이 좋다.\n",
    "# 위 : 예측 값에 대한 지니계수 : 예측값과 실제값으로 구한 지니계수\n",
    "# 아래 : 예측이 완벽할 때의 지니계수 : 실제값과 실제값으로 구한 지니계수\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은 지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n",
    "    # n_samples 개수만큼 1/n_samples ~ 1 사이의 값으로 나눠줌.\n",
    "    \n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기 순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    # np.cumsum(pred_order) : pred_order의 누적합으로 이루어진 배열\n",
    "    # 비율로 표현하기 위해서 np.sum(pred_order)로 나눔.\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "    \n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기 순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측값에 대한 지니계수\n",
    "    \n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fcc8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 하이퍼 파라미터 최적화\n",
    "# 베이지안 최적화 기법을 활용하여 하이퍼파라미터를 조정한다. -> 그리드서치보다 빠르고 효율적이며, 코드도 직관적이다.\n",
    "# c.f. LightGBM은 lgb.Dataset()으로 데이터 셋을 만든다.\n",
    "# 하지만, XGBoost는 xgb.DMatrix()로 데이터 셋을 만든다.\n",
    "\n",
    "# 1. 베이지안 최적화를 위한 데이터 셋을 만든다.\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리함. (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터 셋 - 베이지안 최적화용 데이터셋 만듦.\n",
    "bayes_dtrain_xgb = xgb.DMatrix(X_train, y_train)\n",
    "bayes_dvalid_xgb = xgb.DMatrix(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdfd7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 베이지안 최적화를 위한 데이터 셋을 만든다.\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리함. (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터 셋 - 베이지안 최적화용 데이터셋 만듦.\n",
    "bayes_dtrain_lgb = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid_lgb = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51ea272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 범위 설정 방법\n",
    "# 1. 하이퍼파라미터 범위를 점점 좁히기. 0~1에서 베이지안 최적화 -> 0.5를 하이퍼 파라미터로 찾았다면 0.5 주변에서 베이지안 최적화 ...\n",
    "# 2. 다른 상위권 캐글러가 설정한 하이퍼파라미터를 참고\n",
    "\n",
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위 - 2번 방법으로 직접 가져옴.\n",
    "param_bounds_xgb = {'max_depth': (4, 8),\n",
    "                # 개별 트리의 최대 길이\n",
    "                # 과대적합을 제어하는 파라미터로, 트리의 깊이가 깊을수록 모델이 복잡해지고 과대적합될 우려가 있음.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용.\n",
    "                # 값이 클수록 길이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아짐.\n",
    "                # (값이 클수록 모델 훈련 속도가 느려진다.)\n",
    "                # 기본값 = 6\n",
    "                \n",
    "                'subsample': (0.6, 0.9),\n",
    "                # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율(0~1)\n",
    "                # e.g. 0.5이면 전체 데이터의 50%를 사용하여 트리를 생성함.\n",
    "                # 일반적으로 0.6 ~ 1 사이의 값을 사용. 더 작으면 샘플링할 데이터가 너무 적어짐. \n",
    "                # 기본값 1\n",
    "                \n",
    "                'colsample_bytree': (0.7, 1.0),\n",
    "                # 개별 트리를 훈련할 때, 사용하는 피처 샘플링 비율(0 ~ 1)\n",
    "                # 전체 피처에서 얼마나 샘플링할지를 나타내는 비율\n",
    "                # e.g. 0.7이면 개별 트리를 훈련할 때, 총 피처의 70%만 사용해 훈련함.\n",
    "                # 값이 작을 수록 과대적합 방지 효과가 있음 (보통 0.6 ~ 1 사이의 값 사용함. 기본값 1)\n",
    "                \n",
    "                'min_child_weight': (5, 7),\n",
    "                # 과대적합 방지를 위한 값\n",
    "                # 0 이상으로 설정할 수 있음.\n",
    "                # 값이 클수록 과대적합 방지 효과가 있음.\n",
    "                # 기본값 = 1\n",
    "                \n",
    "                'gamma': (8, 11),\n",
    "                # 말단 노드가 분할하기 위해 필요한 최소 손실 감소 값\n",
    "                # 0 이상의 값으로 설정할 수 있음.\n",
    "                # 손실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있음.\n",
    "                # 기본값 = 0\n",
    "                \n",
    "                'reg_alpha': (7, 9),\n",
    "                # L1 규제 조정값 - 값이 클수록 과대적합 방지 효과 있음. (기본 값 0)\n",
    "                \n",
    "                'reg_lambda': (1.1, 1.5),\n",
    "                # L2 규제 조정값 - 값이 클수록 과대적합 방지 효과 있음. (기본 값 1)\n",
    "                \n",
    "                'scale_pos_weight': (1.4, 1.6)}\n",
    "                # 불균형 데이터 가중치 조정 값\n",
    "                # 타겟값이 불균형 할 때 양성 값(1)에 scale_pos_weight만큼 가중치를 줘서 균형을 맞춤.\n",
    "                # 일반적으로 scale_pos_weight 값을 (음성 타겟값의 개수 / 양성 타겟값 개수)로 설정함.\n",
    "                # 기본값 1\n",
    "\n",
    "# 베이지안 최적화를 수행하면 param_bounds의 하이퍼파라미터 범위를 순회함.\n",
    "# 순회하면서 하이퍼파라미터 값을 적용하여 모델을 훈련하고, 훈련된 모델로 성능을 평가함.\n",
    "# - 목적 : 최고 성능을 낸 하이퍼파라미터를 찾는 것. - 평가 지표 계산 함수는? \n",
    "\n",
    "# 값이 고정된 하이퍼 파라미터\n",
    "fixed_params_xgb = {'objective' : 'binary:logistic', #이진분류 문제이므로,\n",
    "                'learning_rate' : 0.02, # 학습률\n",
    "                'random_state' : 46} # 랜덤 시드값 (코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n",
    "                # c.f. 경우에 따라서(보통 디버깅 등을 위해 ) 동일한 순서로 난수를 발생시켜야 할 경우가 있으므로, 이를 위한 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3e4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 범위 설정 방법\n",
    "# 1. 하이퍼파라미터 범위를 점점 좁히기. 0~1에서 베이지안 최적화 -> 0.5를 하이퍼 파라미터로 찾았다면 0.5 주변에서 베이지안 최적화 ...\n",
    "# 2. 다른 상위권 캐글러가 설정한 하이퍼파라미터를 참고\n",
    "\n",
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위 - 2번 방법으로 직접 가져옴.\n",
    "param_bounds_lgb = {'num_leaves': (30, 40),\n",
    "                # 개별 트리가 가질 수 있는 최대 말단 노드 개수 (제한)\n",
    "                # 트리의 복잡도를 결정하는 주요 파라미터로 값이 클수록 성능이 좋아질 수 있으나 과대적합 우려가 있음. (기본값 31)\n",
    "                \n",
    "                'lambda_l1': (0.7, 0.9),\n",
    "                # L1 규제 조정값 - 값이 클수록 과대적합 방지 효과 있음. (기본 값 0)\n",
    "                \n",
    "                'lambda_l2': (0.9, 1),\n",
    "                # L2 규제 조정값 - 값이 클수록 과대적합 방지 효과 있음. (기본 값 0)\n",
    "                \n",
    "                'feature_fraction': (0.6, 0.7),\n",
    "                # 개별 트리를 훈련할 때, 사용하는 피처 샘플링 비율(0 ~ 1)\n",
    "                # 전체 피처에서 얼마나 샘플링할지를 나타내는 비율\n",
    "                # e.g. 0.7이면 개별 트리를 훈련할 때, 총 피처의 70%만 사용해 훈련함.\n",
    "                # 값이 작을 수록 과대적합 방지 효과가 있음 (보통 0.6 ~ 1 사이의 값 사용함. 기본값 1)\n",
    "                \n",
    "                'bagging_fraction': (0.6, 0.9),\n",
    "                # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율(0~1)\n",
    "                # e.g. 0.5이면 전체 데이터의 50%를 사용하여 트리를 생성함.\n",
    "                # 일반적으로 0.6 ~ 1 사이의 값을 사용. 더 작으면 샘플링할 데이터가 너무 적어짐. \n",
    "                # 기본값 1\n",
    "                \n",
    "                'min_child_samples': (6, 10),\n",
    "                # 과대적합 방지를 위한 값\n",
    "                # 말단 노드가 되기 위해 필요한 최소 데이터 개수\n",
    "                # 기본값 20\n",
    "                \n",
    "                'min_child_weight': (10, 40)}\n",
    "                # 과대적합 방지를 위한 값\n",
    "                # 0 이상으로 설정할 수 있음.\n",
    "                # 값이 클수록 과대적합 방지 효과가 있음.\n",
    "                # 기본값 = 1e-3\n",
    "                \n",
    "# 베이지안 최적화를 수행하면 param_bounds의 하이퍼파라미터 범위를 순회함.\n",
    "# 순회하면서 하이퍼파라미터 값을 적용하여 모델을 훈련하고, 훈련된 모델로 성능을 평가함.\n",
    "# - 목적 : 최고 성능을 낸 하이퍼파라미터를 찾는 것. - 평가 지표 계산 함수는? \n",
    "\n",
    "# 값이 고정된 하이퍼 파라미터\n",
    "fixed_params_lgb = {'objective' : 'binary', #이진분류 문제이므로,\n",
    "                'learning_rate' : 0.005, # 학습률\n",
    "                'bagging_freq' : 1, # 배깅 수행빈도 : 몇 번의 이터레이션마다 배깅을 수행할지 결정함.\n",
    "                # c.f. 배깅(Bootstrap aggregating) : 중복을 허용한 랜덤 샘플링으로 훈련세트를 만들어서 학습.\n",
    "                # 0 : 배깅을 수행하지 않음 (기본 값)\n",
    "                # 1 : 전달 시 매 이터레이션마다 트리가 새로운 샘플링 데이터로 학습함.\n",
    "                'force_row_wise' : True, # 경고 문구 제거용\n",
    "                'random_state' : 46} # 랜덤 시드값 (코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)\n",
    "                # c.f. 경우에 따라서(보통 디버깅 등을 위해 ) 동일한 순서로 난수를 발생시켜야 할 경우가 있으므로, 이를 위한 값."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4f29902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM으로 넘겨주기 위한 용도 : gini() 함수 반환값 3개\n",
    "# XGBoost용 지니계수 계산 함수 반환값 2개 (평가지표명, 평가점수)\n",
    "# - 평가점수가 높으면 좋은지 여부는 XGBoost 모델의 train()에 따로 전달해야한다.\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()  # 데이터값의 타깃값을 반환함.\n",
    "    return 'gini', eval_gini(labels, preds)\n",
    "    # 평가지표 이름, 평가점수\n",
    "    \n",
    "# LightGBM으로 넘겨주기 위한 용도 : gini() 함수\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()  # 데이터값의 타깃값을 반환함.\n",
    "    return 'gini', eval_gini(labels, preds), True\n",
    "    # 평가지표 이름, 평가점수, 평가 점수가 높을수록 좋은지 여부 (여기서는 지니계수가 높을수록 좋으므로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c39c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화용 평가지표 계산 함수 -> 이 함수로 지니계수를 계산해 최적의 하이퍼파라미터를 찾는다.\n",
    "# XGBoost 하이퍼 파라미터를 인수로 받아서 XGBoost를 훈련한 뒤, 지니계수를 반환한다.\n",
    "\n",
    "def eval_function_xgb(max_depth, subsample, colsample_bytree, min_child_weight,\n",
    "                  gamma, reg_alpha, reg_lambda, scale_pos_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "    # 인수로 받은 하이퍼파라미터 값(범위)를 그대로 대입함.\n",
    "    # int(round(@)) : 실수형을 정수형으로 바꿔줌. - num_leaves, min_child_samples는 정수여야함.\n",
    "    # 왜 필요하나? 베이지안 최적화하면 하이퍼파라미터 지정 범위 내 실수값을 탐색하므로 전달하는 값을 정수형으로 바꿔주기 위함.\n",
    "    params = {'max_depth': int(round(max_depth)),\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'reg_alpha':reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight': scale_pos_weight}\n",
    "    \n",
    "    # 고정된 하이퍼 파라미터도 추가 - 우수한 캐글 신의 도움.\n",
    "    # params는 딕셔너리 타입이므로, update() 함수로 추가해줌.\n",
    "    params.update(fixed_params_xgb)\n",
    "    \n",
    "    print('하이퍼 파라미터:', params)\n",
    "    \n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=params, # 훈련용 하이퍼 파라미터\n",
    "                          dtrain=bayes_dtrain_xgb, # 베이지안 훈련 데이터 셋\n",
    "                          num_boost_round=2000, #부스팅 반복 횟수\n",
    "                          # 클수록 성능이 좋아히지나 과대적합 우려.\n",
    "                          # 작을수록 반복횟수가 줄어들어 훈련시간이 짧아짐.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야함.\n",
    "                          evals=[(bayes_dvalid_xgb, 'bayes_dvalid')], #성능 평가용 검증 데이터 셋\n",
    "                          # 검증 데이터를 전달받는 파라미터로, 검증데이터와 검증 데이터 이름의 쌍으로 전달했음.\n",
    "                          maximize=True, # gini로 평가할 때 평가점수가 높으면 좋은지 여부 - True\n",
    "                          feval=gini_xgb, #검증용 평가지표\n",
    "                          early_stopping_rounds=200, #조기종료 조건\n",
    "                          # num_boost_round 만큼 훈련을 반복하는데, 매 이터레이션 마다 eval로 평가시 성능이 연속적으로 좋아지지않으면 중단.\n",
    "                          verbose_eval=100) # 100번째마다 점수 출력 - 출력값이 많아지는 걸 방지.\n",
    "        \n",
    "    best_iter = xgb_model.best_iteration # 최적 반복 횟수\n",
    "    # LightGBM은 기본적으로 훈련 단계에서 성능이 가장 좋았던 반복 횟수 때의 모델을 활용하여 예측하지만,\n",
    "    # XGBoost는 성능이 가장 좋을 때의 부스팅 반복횟수를 명시해줘야함. -> iteration_range()\n",
    "    \n",
    "    # 검증 데이터로 예측 수행 - K개 만큼 나옴.\n",
    "    # XGBoost의 predict는 데이터를 DMatrix 타입으로 전달해야 한다.\n",
    "    preds = xgb_model.predict(bayes_dvalid_xgb, \n",
    "                              iteration_range=(0, best_iter))\n",
    "    # XGBoost는 성능이 가장 좋을 때의 부스팅 반복횟수를 명시해줘야함.\n",
    "    # 따라서 iteration_range 파라미터로 명시해줌. -> 최적의 반복횟수로 훈련된 모델을 활용해 예측함.\n",
    "    \n",
    "    # 지니계수 계산 - 예측값과 검증 데이터 라벨(타겟값)을 이용하여 지니계수를 계산함.\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    # 지니계수 반환.\n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07e2e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화용 평가지표 계산 함수 -> 이 함수로 지니계수를 계산해 최적의 하이퍼파라미터를 찾는다.\n",
    "# LightGBM의 하이퍼파라미터 7개를 인수로 받고 지니계수를 반환한다.\n",
    "\n",
    "def eval_function_lgb(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "    # 인수로 받은 하이퍼파라미터 값(범위)를 그대로 대입함.\n",
    "    # int(round(@)) : 실수형을 정수형으로 바꿔줌. - num_leaves, min_child_samples는 정수여야함.\n",
    "    # 왜 필요하나? 베이지안 최적화하면 하이퍼파라미터 지정 범위 내 실수값을 탐색하므로 전달하는 값을 정수형으로 바꿔주기 위함.\n",
    "    params = {'num_leaves' : int(round(num_leaves)),\n",
    "              'lambda_l1' : lambda_l1,\n",
    "              'lambda_l2' : lambda_l2,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'min_child_samples' : int(round(min_child_samples)),\n",
    "              'min_child_weight' : min_child_weight,              \n",
    "              'feature_pre_filter' : False}\n",
    "    \n",
    "    # 고정된 하이퍼 파라미터도 추가 - 우수한 캐글 신의 도움.\n",
    "    # params는 딕셔너리 타입이므로, update() 함수로 추가해줌.\n",
    "    params.update(fixed_params_lgb)\n",
    "    \n",
    "    print('하이퍼 파라미터:', params)\n",
    "    \n",
    "    # LightGBM 모델 훈련 - 위에서 정의한 하이퍼 파라미터 정의 사용.\n",
    "    lgb_model = lgb.train(params=params, #훈련용 하이퍼 파라미터\n",
    "                          train_set=bayes_dtrain_lgb, # 베이지안 훈련 데이터 셋\n",
    "                          num_boost_round=2500, #부스팅 반복 횟수\n",
    "                          # 클수록 성능이 좋아히지나 과대적합 우려.\n",
    "                          # 작을수록 반복횟수가 줄어들어 훈련시간이 짧아짐.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야함.\n",
    "                          valid_sets=bayes_dvalid_lgb, #성능 평가용 검증 데이터 셋\n",
    "                          feval=gini_lgb, #검증용 평가지표\n",
    "                          early_stopping_rounds=300, #조기종료 조건\n",
    "                          # num_boost_round 만큼 훈련을 반복하는데, 매 이터레이션 마다 eval로 평가시 성능이 연속적으로 좋아지지않으면 중단.\n",
    "                          verbose_eval=100) # 100번째마다 점수 출력 - 출력값이 많아지는 걸 방지.\n",
    "    \n",
    "    \n",
    "    # 검증 데이터로 예측 수행 - K개 만큼 나옴.\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 지니계수 계산 - 예측값과 검증 데이터 라벨(타겟값)을 이용하여 지니계수를 계산함.\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    # 지니계수 반환.\n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ce02e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼 파라미터: {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tbayes_dvalid-logloss:0.67672\tbayes_dvalid-gini:0.17862\n",
      "[100]\tbayes_dvalid-logloss:0.19180\tbayes_dvalid-gini:0.24150\n",
      "[200]\tbayes_dvalid-logloss:0.15821\tbayes_dvalid-gini:0.26629\n",
      "[300]\tbayes_dvalid-logloss:0.15491\tbayes_dvalid-gini:0.27386\n",
      "[400]\tbayes_dvalid-logloss:0.15442\tbayes_dvalid-gini:0.27721\n",
      "[500]\tbayes_dvalid-logloss:0.15432\tbayes_dvalid-gini:0.27871\n",
      "[600]\tbayes_dvalid-logloss:0.15427\tbayes_dvalid-gini:0.27994\n",
      "[700]\tbayes_dvalid-logloss:0.15426\tbayes_dvalid-gini:0.28091\n",
      "[800]\tbayes_dvalid-logloss:0.15422\tbayes_dvalid-gini:0.28164\n",
      "[900]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28211\n",
      "[1000]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28250\n",
      "[1100]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28281\n",
      "[1200]\tbayes_dvalid-logloss:0.15419\tbayes_dvalid-gini:0.28305\n",
      "[1300]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28338\n",
      "[1400]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28342\n",
      "[1500]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28352\n",
      "[1600]\tbayes_dvalid-logloss:0.15417\tbayes_dvalid-gini:0.28379\n",
      "[1700]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28373\n",
      "[1800]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28379\n",
      "[1900]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28386\n",
      "[1999]\tbayes_dvalid-logloss:0.15418\tbayes_dvalid-gini:0.28393\n",
      "지니계수 : 0.2839406943073567\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2839   \u001b[0m | \u001b[0m0.8646   \u001b[0m | \u001b[0m10.15    \u001b[0m | \u001b[0m6.411    \u001b[0m | \u001b[0m6.09     \u001b[0m | \u001b[0m7.847    \u001b[0m | \u001b[0m1.358    \u001b[0m | \u001b[0m1.488    \u001b[0m | \u001b[0m0.8675   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 6.0577898395058085, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67662\tbayes_dvalid-gini:0.14437\n",
      "[100]\tbayes_dvalid-logloss:0.19030\tbayes_dvalid-gini:0.24052\n",
      "[200]\tbayes_dvalid-logloss:0.15719\tbayes_dvalid-gini:0.26746\n",
      "[300]\tbayes_dvalid-logloss:0.15403\tbayes_dvalid-gini:0.27625\n",
      "[400]\tbayes_dvalid-logloss:0.15358\tbayes_dvalid-gini:0.27988\n",
      "[500]\tbayes_dvalid-logloss:0.15345\tbayes_dvalid-gini:0.28188\n",
      "[600]\tbayes_dvalid-logloss:0.15341\tbayes_dvalid-gini:0.28277\n",
      "[700]\tbayes_dvalid-logloss:0.15339\tbayes_dvalid-gini:0.28372\n",
      "[800]\tbayes_dvalid-logloss:0.15340\tbayes_dvalid-gini:0.28400\n",
      "[900]\tbayes_dvalid-logloss:0.15339\tbayes_dvalid-gini:0.28444\n",
      "[1000]\tbayes_dvalid-logloss:0.15337\tbayes_dvalid-gini:0.28472\n",
      "[1100]\tbayes_dvalid-logloss:0.15338\tbayes_dvalid-gini:0.28477\n",
      "[1200]\tbayes_dvalid-logloss:0.15339\tbayes_dvalid-gini:0.28457\n",
      "[1259]\tbayes_dvalid-logloss:0.15340\tbayes_dvalid-gini:0.28464\n",
      "지니계수 : 0.2848735632842038\n",
      "\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.2849   \u001b[0m | \u001b[95m0.9891   \u001b[0m | \u001b[95m9.15     \u001b[0m | \u001b[95m7.167    \u001b[0m | \u001b[95m6.058    \u001b[0m | \u001b[95m8.136    \u001b[0m | \u001b[95m1.47     \u001b[0m | \u001b[95m1.414    \u001b[0m | \u001b[95m0.6261   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 6.7400242964936385, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67673\tbayes_dvalid-gini:0.18082\n",
      "[100]\tbayes_dvalid-logloss:0.19198\tbayes_dvalid-gini:0.24309\n",
      "[200]\tbayes_dvalid-logloss:0.15831\tbayes_dvalid-gini:0.26574\n",
      "[300]\tbayes_dvalid-logloss:0.15498\tbayes_dvalid-gini:0.27383\n",
      "[400]\tbayes_dvalid-logloss:0.15446\tbayes_dvalid-gini:0.27778\n",
      "[500]\tbayes_dvalid-logloss:0.15435\tbayes_dvalid-gini:0.27974\n",
      "[600]\tbayes_dvalid-logloss:0.15430\tbayes_dvalid-gini:0.28061\n",
      "[700]\tbayes_dvalid-logloss:0.15428\tbayes_dvalid-gini:0.28155\n",
      "[800]\tbayes_dvalid-logloss:0.15425\tbayes_dvalid-gini:0.28221\n",
      "[900]\tbayes_dvalid-logloss:0.15422\tbayes_dvalid-gini:0.28286\n",
      "[1000]\tbayes_dvalid-logloss:0.15423\tbayes_dvalid-gini:0.28314\n",
      "[1100]\tbayes_dvalid-logloss:0.15422\tbayes_dvalid-gini:0.28332\n",
      "[1200]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28361\n",
      "[1300]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28375\n",
      "[1400]\tbayes_dvalid-logloss:0.15423\tbayes_dvalid-gini:0.28380\n",
      "[1500]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28386\n",
      "[1600]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28407\n",
      "[1700]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28425\n",
      "[1800]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28430\n",
      "[1900]\tbayes_dvalid-logloss:0.15421\tbayes_dvalid-gini:0.28420\n",
      "[1941]\tbayes_dvalid-logloss:0.15420\tbayes_dvalid-gini:0.28427\n",
      "지니계수 : 0.2843930065481214\n",
      "\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.2844   \u001b[0m | \u001b[0m0.7061   \u001b[0m | \u001b[0m10.5     \u001b[0m | \u001b[0m7.113    \u001b[0m | \u001b[0m6.74     \u001b[0m | \u001b[0m8.957    \u001b[0m | \u001b[0m1.42     \u001b[0m | \u001b[0m1.492    \u001b[0m | \u001b[0m0.8342   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.7001630536555632, 'colsample_bytree': 0.8843124587484356, 'min_child_weight': 6.494091293383359, 'gamma': 10.452246227672624, 'reg_alpha': 8.551838810159788, 'reg_lambda': 1.3814765995549108, 'scale_pos_weight': 1.423280772455086, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67664\tbayes_dvalid-gini:0.12412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tbayes_dvalid-logloss:0.19050\tbayes_dvalid-gini:0.24025\n",
      "[200]\tbayes_dvalid-logloss:0.15734\tbayes_dvalid-gini:0.26652\n",
      "[300]\tbayes_dvalid-logloss:0.15419\tbayes_dvalid-gini:0.27452\n",
      "[400]\tbayes_dvalid-logloss:0.15371\tbayes_dvalid-gini:0.27833\n",
      "[500]\tbayes_dvalid-logloss:0.15359\tbayes_dvalid-gini:0.28038\n",
      "[600]\tbayes_dvalid-logloss:0.15354\tbayes_dvalid-gini:0.28136\n",
      "[700]\tbayes_dvalid-logloss:0.15353\tbayes_dvalid-gini:0.28171\n",
      "[800]\tbayes_dvalid-logloss:0.15352\tbayes_dvalid-gini:0.28199\n",
      "[900]\tbayes_dvalid-logloss:0.15350\tbayes_dvalid-gini:0.28228\n",
      "[1000]\tbayes_dvalid-logloss:0.15350\tbayes_dvalid-gini:0.28267\n",
      "[1100]\tbayes_dvalid-logloss:0.15350\tbayes_dvalid-gini:0.28281\n",
      "[1200]\tbayes_dvalid-logloss:0.15348\tbayes_dvalid-gini:0.28338\n",
      "[1300]\tbayes_dvalid-logloss:0.15348\tbayes_dvalid-gini:0.28336\n",
      "[1400]\tbayes_dvalid-logloss:0.15350\tbayes_dvalid-gini:0.28343\n",
      "[1425]\tbayes_dvalid-logloss:0.15350\tbayes_dvalid-gini:0.28326\n",
      "지니계수 : 0.28349110744810135\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2835   \u001b[0m | \u001b[0m0.8843   \u001b[0m | \u001b[0m10.45    \u001b[0m | \u001b[0m6.838    \u001b[0m | \u001b[0m6.494    \u001b[0m | \u001b[0m8.552    \u001b[0m | \u001b[0m1.381    \u001b[0m | \u001b[0m1.423    \u001b[0m | \u001b[0m0.7002   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.8535233675350644, 'colsample_bytree': 0.92975858050776, 'min_child_weight': 6.249564429359247, 'gamma': 9.95563546750357, 'reg_alpha': 8.411512219837842, 'reg_lambda': 1.424460008293778, 'scale_pos_weight': 1.5416807226581535, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tbayes_dvalid-logloss:0.67679\tbayes_dvalid-gini:0.15322\n",
      "[100]\tbayes_dvalid-logloss:0.19298\tbayes_dvalid-gini:0.24349\n",
      "[200]\tbayes_dvalid-logloss:0.15895\tbayes_dvalid-gini:0.26799\n",
      "[300]\tbayes_dvalid-logloss:0.15549\tbayes_dvalid-gini:0.27636\n",
      "[400]\tbayes_dvalid-logloss:0.15500\tbayes_dvalid-gini:0.27922\n",
      "[500]\tbayes_dvalid-logloss:0.15488\tbayes_dvalid-gini:0.28113\n",
      "[600]\tbayes_dvalid-logloss:0.15484\tbayes_dvalid-gini:0.28193\n",
      "[700]\tbayes_dvalid-logloss:0.15484\tbayes_dvalid-gini:0.28220\n",
      "[800]\tbayes_dvalid-logloss:0.15482\tbayes_dvalid-gini:0.28269\n",
      "[900]\tbayes_dvalid-logloss:0.15479\tbayes_dvalid-gini:0.28323\n",
      "[1000]\tbayes_dvalid-logloss:0.15479\tbayes_dvalid-gini:0.28355\n",
      "[1100]\tbayes_dvalid-logloss:0.15479\tbayes_dvalid-gini:0.28390\n",
      "[1200]\tbayes_dvalid-logloss:0.15478\tbayes_dvalid-gini:0.28403\n",
      "[1300]\tbayes_dvalid-logloss:0.15479\tbayes_dvalid-gini:0.28397\n",
      "[1400]\tbayes_dvalid-logloss:0.15481\tbayes_dvalid-gini:0.28384\n",
      "[1442]\tbayes_dvalid-logloss:0.15481\tbayes_dvalid-gini:0.28362\n",
      "지니계수 : 0.2840660861183519\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.2841   \u001b[0m | \u001b[0m0.9298   \u001b[0m | \u001b[0m9.956    \u001b[0m | \u001b[0m6.809    \u001b[0m | \u001b[0m6.25     \u001b[0m | \u001b[0m8.412    \u001b[0m | \u001b[0m1.424    \u001b[0m | \u001b[0m1.542    \u001b[0m | \u001b[0m0.8535   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.6462619019069298, 'colsample_bytree': 0.80929192865947, 'min_child_weight': 6.079999276892042, 'gamma': 9.553916776586505, 'reg_alpha': 8.860396362258099, 'reg_lambda': 1.4050740023119348, 'scale_pos_weight': 1.4668544695338273, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67669\tbayes_dvalid-gini:0.12412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tbayes_dvalid-logloss:0.19146\tbayes_dvalid-gini:0.24171\n",
      "[200]\tbayes_dvalid-logloss:0.15794\tbayes_dvalid-gini:0.26715\n",
      "[300]\tbayes_dvalid-logloss:0.15465\tbayes_dvalid-gini:0.27607\n",
      "[400]\tbayes_dvalid-logloss:0.15416\tbayes_dvalid-gini:0.27966\n",
      "[500]\tbayes_dvalid-logloss:0.15401\tbayes_dvalid-gini:0.28202\n",
      "[600]\tbayes_dvalid-logloss:0.15396\tbayes_dvalid-gini:0.28309\n",
      "[700]\tbayes_dvalid-logloss:0.15395\tbayes_dvalid-gini:0.28383\n",
      "[800]\tbayes_dvalid-logloss:0.15394\tbayes_dvalid-gini:0.28402\n",
      "[900]\tbayes_dvalid-logloss:0.15393\tbayes_dvalid-gini:0.28424\n",
      "[1000]\tbayes_dvalid-logloss:0.15391\tbayes_dvalid-gini:0.28482\n",
      "[1100]\tbayes_dvalid-logloss:0.15391\tbayes_dvalid-gini:0.28496\n",
      "[1200]\tbayes_dvalid-logloss:0.15392\tbayes_dvalid-gini:0.28484\n",
      "[1300]\tbayes_dvalid-logloss:0.15391\tbayes_dvalid-gini:0.28497\n",
      "[1330]\tbayes_dvalid-logloss:0.15391\tbayes_dvalid-gini:0.28492\n",
      "지니계수 : 0.2851505938666964\n",
      "\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.2852   \u001b[0m | \u001b[95m0.8093   \u001b[0m | \u001b[95m9.554    \u001b[0m | \u001b[95m6.532    \u001b[0m | \u001b[95m6.08     \u001b[0m | \u001b[95m8.86     \u001b[0m | \u001b[95m1.405    \u001b[0m | \u001b[95m1.467    \u001b[0m | \u001b[95m0.6463   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.6931141936797243, 'colsample_bytree': 0.8817801730078565, 'min_child_weight': 6.992334203641873, 'gamma': 9.013424730095146, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tbayes_dvalid-logloss:0.67667\tbayes_dvalid-gini:0.13101\n",
      "[100]\tbayes_dvalid-logloss:0.19100\tbayes_dvalid-gini:0.24335\n",
      "[200]\tbayes_dvalid-logloss:0.15763\tbayes_dvalid-gini:0.26800\n",
      "[300]\tbayes_dvalid-logloss:0.15439\tbayes_dvalid-gini:0.27721\n",
      "[400]\tbayes_dvalid-logloss:0.15391\tbayes_dvalid-gini:0.28081\n",
      "[500]\tbayes_dvalid-logloss:0.15379\tbayes_dvalid-gini:0.28282\n",
      "[600]\tbayes_dvalid-logloss:0.15376\tbayes_dvalid-gini:0.28337\n",
      "[700]\tbayes_dvalid-logloss:0.15376\tbayes_dvalid-gini:0.28405\n",
      "[800]\tbayes_dvalid-logloss:0.15374\tbayes_dvalid-gini:0.28449\n",
      "[900]\tbayes_dvalid-logloss:0.15372\tbayes_dvalid-gini:0.28486\n",
      "[1000]\tbayes_dvalid-logloss:0.15371\tbayes_dvalid-gini:0.28522\n",
      "[1100]\tbayes_dvalid-logloss:0.15374\tbayes_dvalid-gini:0.28478\n",
      "[1200]\tbayes_dvalid-logloss:0.15375\tbayes_dvalid-gini:0.28462\n",
      "[1208]\tbayes_dvalid-logloss:0.15376\tbayes_dvalid-gini:0.28469\n",
      "지니계수 : 0.2852042280808545\n",
      "\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.2852   \u001b[0m | \u001b[95m0.8818   \u001b[0m | \u001b[95m9.013    \u001b[0m | \u001b[95m6.927    \u001b[0m | \u001b[95m6.992    \u001b[0m | \u001b[95m7.641    \u001b[0m | \u001b[95m1.356    \u001b[0m | \u001b[95m1.449    \u001b[0m | \u001b[95m0.6931   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 7, 'subsample': 0.622178673498624, 'colsample_bytree': 0.8038194271761055, 'min_child_weight': 6.660865577720352, 'gamma': 8.900382878172401, 'reg_alpha': 8.269972366729917, 'reg_lambda': 1.3722631998811472, 'scale_pos_weight': 1.4512147328253817, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67667\tbayes_dvalid-gini:0.12652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tbayes_dvalid-logloss:0.19111\tbayes_dvalid-gini:0.24334\n",
      "[200]\tbayes_dvalid-logloss:0.15769\tbayes_dvalid-gini:0.26772\n",
      "[300]\tbayes_dvalid-logloss:0.15444\tbayes_dvalid-gini:0.27647\n",
      "[400]\tbayes_dvalid-logloss:0.15398\tbayes_dvalid-gini:0.28012\n",
      "[500]\tbayes_dvalid-logloss:0.15383\tbayes_dvalid-gini:0.28215\n",
      "[600]\tbayes_dvalid-logloss:0.15379\tbayes_dvalid-gini:0.28303\n",
      "[700]\tbayes_dvalid-logloss:0.15377\tbayes_dvalid-gini:0.28400\n",
      "[800]\tbayes_dvalid-logloss:0.15378\tbayes_dvalid-gini:0.28415\n",
      "[900]\tbayes_dvalid-logloss:0.15376\tbayes_dvalid-gini:0.28450\n",
      "[1000]\tbayes_dvalid-logloss:0.15375\tbayes_dvalid-gini:0.28503\n",
      "[1100]\tbayes_dvalid-logloss:0.15375\tbayes_dvalid-gini:0.28488\n",
      "[1200]\tbayes_dvalid-logloss:0.15375\tbayes_dvalid-gini:0.28494\n",
      "[1238]\tbayes_dvalid-logloss:0.15376\tbayes_dvalid-gini:0.28480\n",
      "지니계수 : 0.285161899752009\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2852   \u001b[0m | \u001b[0m0.8038   \u001b[0m | \u001b[0m8.9      \u001b[0m | \u001b[0m6.698    \u001b[0m | \u001b[0m6.661    \u001b[0m | \u001b[0m8.27     \u001b[0m | \u001b[0m1.372    \u001b[0m | \u001b[0m1.451    \u001b[0m | \u001b[0m0.6222   \u001b[0m |\n",
      "하이퍼 파라미터: {'max_depth': 6, 'subsample': 0.6718906735279808, 'colsample_bytree': 0.7483650710705307, 'min_child_weight': 5.958951716821881, 'gamma': 8.7452856635238, 'reg_alpha': 9.0, 'reg_lambda': 1.4408571168437925, 'scale_pos_weight': 1.5373195925858119, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n",
      "[0]\tbayes_dvalid-logloss:0.67677\tbayes_dvalid-gini:0.12975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tbayes_dvalid-logloss:0.19294\tbayes_dvalid-gini:0.24419\n",
      "[200]\tbayes_dvalid-logloss:0.15896\tbayes_dvalid-gini:0.26608\n",
      "[300]\tbayes_dvalid-logloss:0.15551\tbayes_dvalid-gini:0.27490\n",
      "[400]\tbayes_dvalid-logloss:0.15499\tbayes_dvalid-gini:0.27880\n",
      "[500]\tbayes_dvalid-logloss:0.15485\tbayes_dvalid-gini:0.28092\n",
      "[600]\tbayes_dvalid-logloss:0.15480\tbayes_dvalid-gini:0.28210\n",
      "[700]\tbayes_dvalid-logloss:0.15479\tbayes_dvalid-gini:0.28293\n",
      "[800]\tbayes_dvalid-logloss:0.15476\tbayes_dvalid-gini:0.28379\n",
      "[900]\tbayes_dvalid-logloss:0.15474\tbayes_dvalid-gini:0.28405\n",
      "[1000]\tbayes_dvalid-logloss:0.15474\tbayes_dvalid-gini:0.28430\n",
      "[1100]\tbayes_dvalid-logloss:0.15475\tbayes_dvalid-gini:0.28441\n",
      "[1200]\tbayes_dvalid-logloss:0.15475\tbayes_dvalid-gini:0.28452\n",
      "[1300]\tbayes_dvalid-logloss:0.15472\tbayes_dvalid-gini:0.28488\n",
      "[1400]\tbayes_dvalid-logloss:0.15475\tbayes_dvalid-gini:0.28496\n",
      "[1500]\tbayes_dvalid-logloss:0.15474\tbayes_dvalid-gini:0.28489\n",
      "[1600]\tbayes_dvalid-logloss:0.15474\tbayes_dvalid-gini:0.28492\n",
      "[1655]\tbayes_dvalid-logloss:0.15474\tbayes_dvalid-gini:0.28476\n",
      "지니계수 : 0.28516887899308535\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.2852   \u001b[0m | \u001b[0m0.7484   \u001b[0m | \u001b[0m8.745    \u001b[0m | \u001b[0m6.452    \u001b[0m | \u001b[0m5.959    \u001b[0m | \u001b[0m9.0      \u001b[0m | \u001b[0m1.441    \u001b[0m | \u001b[0m1.537    \u001b[0m | \u001b[0m0.6719   \u001b[0m |\n",
      "=========================================================================================================================\n",
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼 파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153615\tvalid_0's gini: 0.253691\n",
      "[200]\tvalid_0's binary_logloss: 0.152615\tvalid_0's gini: 0.259934\n",
      "[300]\tvalid_0's binary_logloss: 0.152059\tvalid_0's gini: 0.265371\n",
      "[400]\tvalid_0's binary_logloss: 0.151722\tvalid_0's gini: 0.269501\n",
      "[500]\tvalid_0's binary_logloss: 0.151507\tvalid_0's gini: 0.272236\n",
      "[600]\tvalid_0's binary_logloss: 0.151366\tvalid_0's gini: 0.274544\n",
      "[700]\tvalid_0's binary_logloss: 0.15127\tvalid_0's gini: 0.276229\n",
      "[800]\tvalid_0's binary_logloss: 0.151203\tvalid_0's gini: 0.277657\n",
      "[900]\tvalid_0's binary_logloss: 0.151141\tvalid_0's gini: 0.279072\n",
      "[1000]\tvalid_0's binary_logloss: 0.1511\tvalid_0's gini: 0.280156\n",
      "[1100]\tvalid_0's binary_logloss: 0.151073\tvalid_0's gini: 0.280801\n",
      "[1200]\tvalid_0's binary_logloss: 0.151048\tvalid_0's gini: 0.281576\n",
      "[1300]\tvalid_0's binary_logloss: 0.151025\tvalid_0's gini: 0.282245\n",
      "[1400]\tvalid_0's binary_logloss: 0.151011\tvalid_0's gini: 0.28266\n",
      "[1500]\tvalid_0's binary_logloss: 0.150998\tvalid_0's gini: 0.283026\n",
      "[1600]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.283393\n",
      "[1700]\tvalid_0's binary_logloss: 0.150979\tvalid_0's gini: 0.283706\n",
      "[1800]\tvalid_0's binary_logloss: 0.150971\tvalid_0's gini: 0.283946\n",
      "[1900]\tvalid_0's binary_logloss: 0.150966\tvalid_0's gini: 0.284222\n",
      "[2000]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.284435\n",
      "[2100]\tvalid_0's binary_logloss: 0.150957\tvalid_0's gini: 0.284639\n",
      "[2200]\tvalid_0's binary_logloss: 0.150962\tvalid_0's gini: 0.284595\n",
      "[2300]\tvalid_0's binary_logloss: 0.15096\tvalid_0's gini: 0.284679\n",
      "Early stopping, best iteration is:\n",
      "[2096]\tvalid_0's binary_logloss: 0.150957\tvalid_0's gini: 0.284665\n",
      "지니계수 : 0.28466512416542983\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2847   \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m0.8206   \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m7.695    \u001b[0m | \u001b[0m29.38    \u001b[0m | \u001b[0m34.38    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153586\tvalid_0's gini: 0.25276\n",
      "[200]\tvalid_0's binary_logloss: 0.152582\tvalid_0's gini: 0.259396\n",
      "[300]\tvalid_0's binary_logloss: 0.152025\tvalid_0's gini: 0.265269\n",
      "[400]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.269604\n",
      "[500]\tvalid_0's binary_logloss: 0.151487\tvalid_0's gini: 0.272512\n",
      "[600]\tvalid_0's binary_logloss: 0.151352\tvalid_0's gini: 0.274672\n",
      "[700]\tvalid_0's binary_logloss: 0.151266\tvalid_0's gini: 0.276081\n",
      "[800]\tvalid_0's binary_logloss: 0.15119\tvalid_0's gini: 0.277692\n",
      "[900]\tvalid_0's binary_logloss: 0.151142\tvalid_0's gini: 0.278814\n",
      "[1000]\tvalid_0's binary_logloss: 0.151107\tvalid_0's gini: 0.279674\n",
      "[1100]\tvalid_0's binary_logloss: 0.151083\tvalid_0's gini: 0.280344\n",
      "[1200]\tvalid_0's binary_logloss: 0.151067\tvalid_0's gini: 0.280941\n",
      "[1300]\tvalid_0's binary_logloss: 0.151053\tvalid_0's gini: 0.281307\n",
      "[1400]\tvalid_0's binary_logloss: 0.151038\tvalid_0's gini: 0.281802\n",
      "[1500]\tvalid_0's binary_logloss: 0.151027\tvalid_0's gini: 0.282183\n",
      "[1600]\tvalid_0's binary_logloss: 0.151025\tvalid_0's gini: 0.282361\n",
      "[1700]\tvalid_0's binary_logloss: 0.151018\tvalid_0's gini: 0.282586\n",
      "[1800]\tvalid_0's binary_logloss: 0.151015\tvalid_0's gini: 0.282809\n",
      "[1900]\tvalid_0's binary_logloss: 0.151012\tvalid_0's gini: 0.283007\n",
      "[2000]\tvalid_0's binary_logloss: 0.151013\tvalid_0's gini: 0.283012\n",
      "[2100]\tvalid_0's binary_logloss: 0.151008\tvalid_0's gini: 0.283275\n",
      "[2200]\tvalid_0's binary_logloss: 0.151001\tvalid_0's gini: 0.283553\n",
      "[2300]\tvalid_0's binary_logloss: 0.151\tvalid_0's gini: 0.283666\n",
      "[2400]\tvalid_0's binary_logloss: 0.151005\tvalid_0's gini: 0.283672\n",
      "[2500]\tvalid_0's binary_logloss: 0.151011\tvalid_0's gini: 0.283603\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.150999\tvalid_0's gini: 0.283678\n",
      "지니계수 : 0.2836782287459284\n",
      "\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2837   \u001b[0m | \u001b[0m0.8675   \u001b[0m | \u001b[0m0.6964   \u001b[0m | \u001b[0m0.7767   \u001b[0m | \u001b[0m0.9792   \u001b[0m | \u001b[0m8.116    \u001b[0m | \u001b[0m27.04    \u001b[0m | \u001b[0m39.26    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153579\tvalid_0's gini: 0.257783\n",
      "[200]\tvalid_0's binary_logloss: 0.152567\tvalid_0's gini: 0.262569\n",
      "[300]\tvalid_0's binary_logloss: 0.152001\tvalid_0's gini: 0.267822\n",
      "[400]\tvalid_0's binary_logloss: 0.15167\tvalid_0's gini: 0.271149\n",
      "[500]\tvalid_0's binary_logloss: 0.151472\tvalid_0's gini: 0.273637\n",
      "[600]\tvalid_0's binary_logloss: 0.151342\tvalid_0's gini: 0.275661\n",
      "[700]\tvalid_0's binary_logloss: 0.151252\tvalid_0's gini: 0.27726\n",
      "[800]\tvalid_0's binary_logloss: 0.151179\tvalid_0's gini: 0.278727\n",
      "[900]\tvalid_0's binary_logloss: 0.151126\tvalid_0's gini: 0.279887\n",
      "[1000]\tvalid_0's binary_logloss: 0.151083\tvalid_0's gini: 0.281012\n",
      "[1100]\tvalid_0's binary_logloss: 0.151046\tvalid_0's gini: 0.281953\n",
      "[1200]\tvalid_0's binary_logloss: 0.151012\tvalid_0's gini: 0.282945\n",
      "[1300]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.28366\n",
      "[1400]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.284017\n",
      "[1500]\tvalid_0's binary_logloss: 0.150975\tvalid_0's gini: 0.28419\n",
      "[1600]\tvalid_0's binary_logloss: 0.150965\tvalid_0's gini: 0.284469\n",
      "[1700]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.284761\n",
      "[1800]\tvalid_0's binary_logloss: 0.150955\tvalid_0's gini: 0.284937\n",
      "[1900]\tvalid_0's binary_logloss: 0.150949\tvalid_0's gini: 0.285267\n",
      "[2000]\tvalid_0's binary_logloss: 0.150945\tvalid_0's gini: 0.285455\n",
      "[2100]\tvalid_0's binary_logloss: 0.150945\tvalid_0's gini: 0.285494\n",
      "[2200]\tvalid_0's binary_logloss: 0.15095\tvalid_0's gini: 0.285441\n",
      "[2300]\tvalid_0's binary_logloss: 0.150946\tvalid_0's gini: 0.285621\n",
      "[2400]\tvalid_0's binary_logloss: 0.150947\tvalid_0's gini: 0.285682\n",
      "[2500]\tvalid_0's binary_logloss: 0.150953\tvalid_0's gini: 0.285543\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2364]\tvalid_0's binary_logloss: 0.15094\tvalid_0's gini: 0.285899\n",
      "지니계수 : 0.2858991477990811\n",
      "\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.2859   \u001b[0m | \u001b[95m0.6213   \u001b[0m | \u001b[95m0.6087   \u001b[0m | \u001b[95m0.704    \u001b[0m | \u001b[95m0.9833   \u001b[0m | \u001b[95m9.113    \u001b[0m | \u001b[95m36.1     \u001b[0m | \u001b[95m39.79    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 40, 'lambda_l1': 0.7182350380170257, 'lambda_l2': 0.9380507539791323, 'feature_fraction': 0.6332171534896, 'bagging_fraction': 0.7635858296673824, 'min_child_samples': 8, 'min_child_weight': 38.16279326527362, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153578\tvalid_0's gini: 0.256726\n",
      "[200]\tvalid_0's binary_logloss: 0.152571\tvalid_0's gini: 0.261669\n",
      "[300]\tvalid_0's binary_logloss: 0.152011\tvalid_0's gini: 0.267128\n",
      "[400]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.270709\n",
      "[500]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.273642\n",
      "[600]\tvalid_0's binary_logloss: 0.151335\tvalid_0's gini: 0.275551\n",
      "[700]\tvalid_0's binary_logloss: 0.151242\tvalid_0's gini: 0.277159\n",
      "[800]\tvalid_0's binary_logloss: 0.151175\tvalid_0's gini: 0.278514\n",
      "[900]\tvalid_0's binary_logloss: 0.151124\tvalid_0's gini: 0.279659\n",
      "[1000]\tvalid_0's binary_logloss: 0.151081\tvalid_0's gini: 0.280834\n",
      "[1100]\tvalid_0's binary_logloss: 0.151049\tvalid_0's gini: 0.281638\n",
      "[1200]\tvalid_0's binary_logloss: 0.151024\tvalid_0's gini: 0.282321\n",
      "[1300]\tvalid_0's binary_logloss: 0.151007\tvalid_0's gini: 0.282797\n",
      "[1400]\tvalid_0's binary_logloss: 0.150997\tvalid_0's gini: 0.283125\n",
      "[1500]\tvalid_0's binary_logloss: 0.150985\tvalid_0's gini: 0.28355\n",
      "[1600]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.283947\n",
      "[1700]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284231\n",
      "[1800]\tvalid_0's binary_logloss: 0.150967\tvalid_0's gini: 0.284398\n",
      "[1900]\tvalid_0's binary_logloss: 0.150961\tvalid_0's gini: 0.284712\n",
      "[2000]\tvalid_0's binary_logloss: 0.150965\tvalid_0's gini: 0.284673\n",
      "[2100]\tvalid_0's binary_logloss: 0.150964\tvalid_0's gini: 0.284824\n",
      "Early stopping, best iteration is:\n",
      "[1895]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.284725\n",
      "지니계수 : 0.28472499359959924\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2847   \u001b[0m | \u001b[0m0.7636   \u001b[0m | \u001b[0m0.6332   \u001b[0m | \u001b[0m0.7182   \u001b[0m | \u001b[0m0.9381   \u001b[0m | \u001b[0m8.348    \u001b[0m | \u001b[0m38.16    \u001b[0m | \u001b[0m39.59    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.153592\tvalid_0's gini: 0.254698\n",
      "[200]\tvalid_0's binary_logloss: 0.15259\tvalid_0's gini: 0.260661\n",
      "[300]\tvalid_0's binary_logloss: 0.15203\tvalid_0's gini: 0.266307\n",
      "[400]\tvalid_0's binary_logloss: 0.151697\tvalid_0's gini: 0.270213\n",
      "[500]\tvalid_0's binary_logloss: 0.151485\tvalid_0's gini: 0.273025\n",
      "[600]\tvalid_0's binary_logloss: 0.151344\tvalid_0's gini: 0.275316\n",
      "[700]\tvalid_0's binary_logloss: 0.151254\tvalid_0's gini: 0.276937\n",
      "[800]\tvalid_0's binary_logloss: 0.151183\tvalid_0's gini: 0.278531\n",
      "[900]\tvalid_0's binary_logloss: 0.151124\tvalid_0's gini: 0.279808\n",
      "[1000]\tvalid_0's binary_logloss: 0.151083\tvalid_0's gini: 0.280864\n",
      "[1100]\tvalid_0's binary_logloss: 0.151055\tvalid_0's gini: 0.281522\n",
      "[1200]\tvalid_0's binary_logloss: 0.151032\tvalid_0's gini: 0.282161\n",
      "[1300]\tvalid_0's binary_logloss: 0.15101\tvalid_0's gini: 0.282797\n",
      "[1400]\tvalid_0's binary_logloss: 0.151005\tvalid_0's gini: 0.283008\n",
      "[1500]\tvalid_0's binary_logloss: 0.150991\tvalid_0's gini: 0.28342\n",
      "[1600]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.283572\n",
      "[1700]\tvalid_0's binary_logloss: 0.150981\tvalid_0's gini: 0.283857\n",
      "[1800]\tvalid_0's binary_logloss: 0.150975\tvalid_0's gini: 0.284041\n",
      "[1900]\tvalid_0's binary_logloss: 0.150972\tvalid_0's gini: 0.284283\n",
      "[2000]\tvalid_0's binary_logloss: 0.150971\tvalid_0's gini: 0.284418\n",
      "[2100]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284573\n",
      "[2200]\tvalid_0's binary_logloss: 0.150971\tvalid_0's gini: 0.284604\n",
      "[2300]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284728\n",
      "[2400]\tvalid_0's binary_logloss: 0.150973\tvalid_0's gini: 0.284655\n",
      "[2500]\tvalid_0's binary_logloss: 0.150973\tvalid_0's gini: 0.284724\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2320]\tvalid_0's binary_logloss: 0.150967\tvalid_0's gini: 0.284788\n",
      "지니계수 : 0.28478843352616184\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.2848   \u001b[0m | \u001b[0m0.7667   \u001b[0m | \u001b[0m0.6606   \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m0.9033   \u001b[0m | \u001b[0m8.769    \u001b[0m | \u001b[0m29.31    \u001b[0m | \u001b[0m36.6     \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 40, 'lambda_l1': 0.7113567244294035, 'lambda_l2': 0.9992148463611682, 'feature_fraction': 0.6823972673568225, 'bagging_fraction': 0.6452323984860321, 'min_child_samples': 9, 'min_child_weight': 36.23198396337493, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15356\tvalid_0's gini: 0.256841\n",
      "[200]\tvalid_0's binary_logloss: 0.152552\tvalid_0's gini: 0.262338\n",
      "[300]\tvalid_0's binary_logloss: 0.151991\tvalid_0's gini: 0.267582\n",
      "[400]\tvalid_0's binary_logloss: 0.151655\tvalid_0's gini: 0.271393\n",
      "[500]\tvalid_0's binary_logloss: 0.151449\tvalid_0's gini: 0.274195\n",
      "[600]\tvalid_0's binary_logloss: 0.151313\tvalid_0's gini: 0.276344\n",
      "[700]\tvalid_0's binary_logloss: 0.151222\tvalid_0's gini: 0.277919\n",
      "[800]\tvalid_0's binary_logloss: 0.151154\tvalid_0's gini: 0.279309\n",
      "[900]\tvalid_0's binary_logloss: 0.151099\tvalid_0's gini: 0.280693\n",
      "[1000]\tvalid_0's binary_logloss: 0.151059\tvalid_0's gini: 0.281776\n",
      "[1100]\tvalid_0's binary_logloss: 0.151031\tvalid_0's gini: 0.282478\n",
      "[1200]\tvalid_0's binary_logloss: 0.151008\tvalid_0's gini: 0.283165\n",
      "[1300]\tvalid_0's binary_logloss: 0.151001\tvalid_0's gini: 0.283417\n",
      "[1400]\tvalid_0's binary_logloss: 0.150987\tvalid_0's gini: 0.283851\n",
      "[1500]\tvalid_0's binary_logloss: 0.150979\tvalid_0's gini: 0.284144\n",
      "[1600]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.284788\n",
      "[1700]\tvalid_0's binary_logloss: 0.150966\tvalid_0's gini: 0.28495\n",
      "[1800]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.284929\n",
      "Early stopping, best iteration is:\n",
      "[1599]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.284795\n",
      "지니계수 : 0.2847945959615039\n",
      "\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2848   \u001b[0m | \u001b[0m0.6452   \u001b[0m | \u001b[0m0.6824   \u001b[0m | \u001b[0m0.7114   \u001b[0m | \u001b[0m0.9992   \u001b[0m | \u001b[0m9.083    \u001b[0m | \u001b[0m36.23    \u001b[0m | \u001b[0m39.59    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 36, 'lambda_l1': 0.862092450221885, 'lambda_l2': 0.900511488097541, 'feature_fraction': 0.6556247435580953, 'bagging_fraction': 0.7236613926476847, 'min_child_samples': 8, 'min_child_weight': 16.800997543304433, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153594\tvalid_0's gini: 0.254613\n",
      "[200]\tvalid_0's binary_logloss: 0.152595\tvalid_0's gini: 0.260597\n",
      "[300]\tvalid_0's binary_logloss: 0.152038\tvalid_0's gini: 0.265938\n",
      "[400]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.269952\n",
      "[500]\tvalid_0's binary_logloss: 0.151491\tvalid_0's gini: 0.272663\n",
      "[600]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.274651\n",
      "[700]\tvalid_0's binary_logloss: 0.151267\tvalid_0's gini: 0.276419\n",
      "[800]\tvalid_0's binary_logloss: 0.151198\tvalid_0's gini: 0.27776\n",
      "[900]\tvalid_0's binary_logloss: 0.151138\tvalid_0's gini: 0.279166\n",
      "[1000]\tvalid_0's binary_logloss: 0.151096\tvalid_0's gini: 0.280184\n",
      "[1100]\tvalid_0's binary_logloss: 0.151066\tvalid_0's gini: 0.280963\n",
      "[1200]\tvalid_0's binary_logloss: 0.151045\tvalid_0's gini: 0.28158\n",
      "[1300]\tvalid_0's binary_logloss: 0.151026\tvalid_0's gini: 0.282104\n",
      "[1400]\tvalid_0's binary_logloss: 0.151016\tvalid_0's gini: 0.282407\n",
      "[1500]\tvalid_0's binary_logloss: 0.150998\tvalid_0's gini: 0.283026\n",
      "[1600]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.283322\n",
      "[1700]\tvalid_0's binary_logloss: 0.150981\tvalid_0's gini: 0.283665\n",
      "[1800]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.28407\n",
      "[1900]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284113\n",
      "[2000]\tvalid_0's binary_logloss: 0.150964\tvalid_0's gini: 0.284371\n",
      "[2100]\tvalid_0's binary_logloss: 0.150954\tvalid_0's gini: 0.284758\n",
      "[2200]\tvalid_0's binary_logloss: 0.150957\tvalid_0's gini: 0.284787\n",
      "[2300]\tvalid_0's binary_logloss: 0.150954\tvalid_0's gini: 0.284942\n",
      "[2400]\tvalid_0's binary_logloss: 0.150953\tvalid_0's gini: 0.28511\n",
      "[2500]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.285139\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2340]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.285102\n",
      "지니계수 : 0.28510249128741233\n",
      "\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2851   \u001b[0m | \u001b[0m0.7237   \u001b[0m | \u001b[0m0.6556   \u001b[0m | \u001b[0m0.8621   \u001b[0m | \u001b[0m0.9005   \u001b[0m | \u001b[0m8.223    \u001b[0m | \u001b[0m16.8     \u001b[0m | \u001b[0m36.38    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 32, 'lambda_l1': 0.829932662510154, 'lambda_l2': 0.9309382272206734, 'feature_fraction': 0.6952194054160209, 'bagging_fraction': 0.6675659367709144, 'min_child_samples': 8, 'min_child_weight': 28.604068940720285, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.153618\tvalid_0's gini: 0.253212\n",
      "[200]\tvalid_0's binary_logloss: 0.152621\tvalid_0's gini: 0.26003\n",
      "[300]\tvalid_0's binary_logloss: 0.152058\tvalid_0's gini: 0.265675\n",
      "[400]\tvalid_0's binary_logloss: 0.15172\tvalid_0's gini: 0.269533\n",
      "[500]\tvalid_0's binary_logloss: 0.151508\tvalid_0's gini: 0.272464\n",
      "[600]\tvalid_0's binary_logloss: 0.15137\tvalid_0's gini: 0.274674\n",
      "[700]\tvalid_0's binary_logloss: 0.15127\tvalid_0's gini: 0.276538\n",
      "[800]\tvalid_0's binary_logloss: 0.151195\tvalid_0's gini: 0.278056\n",
      "[900]\tvalid_0's binary_logloss: 0.151131\tvalid_0's gini: 0.279483\n",
      "[1000]\tvalid_0's binary_logloss: 0.151083\tvalid_0's gini: 0.280689\n",
      "[1100]\tvalid_0's binary_logloss: 0.151052\tvalid_0's gini: 0.281509\n",
      "[1200]\tvalid_0's binary_logloss: 0.151026\tvalid_0's gini: 0.282244\n",
      "[1300]\tvalid_0's binary_logloss: 0.151007\tvalid_0's gini: 0.282763\n",
      "[1400]\tvalid_0's binary_logloss: 0.150996\tvalid_0's gini: 0.283189\n",
      "[1500]\tvalid_0's binary_logloss: 0.150982\tvalid_0's gini: 0.283672\n",
      "[1600]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284125\n",
      "[1700]\tvalid_0's binary_logloss: 0.150957\tvalid_0's gini: 0.284562\n",
      "[1800]\tvalid_0's binary_logloss: 0.150951\tvalid_0's gini: 0.284788\n",
      "[1900]\tvalid_0's binary_logloss: 0.150944\tvalid_0's gini: 0.285061\n",
      "[2000]\tvalid_0's binary_logloss: 0.150943\tvalid_0's gini: 0.285266\n",
      "[2100]\tvalid_0's binary_logloss: 0.150934\tvalid_0's gini: 0.285676\n",
      "[2200]\tvalid_0's binary_logloss: 0.150939\tvalid_0's gini: 0.285666\n",
      "[2300]\tvalid_0's binary_logloss: 0.150938\tvalid_0's gini: 0.285788\n",
      "[2400]\tvalid_0's binary_logloss: 0.150938\tvalid_0's gini: 0.285834\n",
      "Early stopping, best iteration is:\n",
      "[2109]\tvalid_0's binary_logloss: 0.150934\tvalid_0's gini: 0.285685\n",
      "지니계수 : 0.28568536709429204\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2857   \u001b[0m | \u001b[0m0.6676   \u001b[0m | \u001b[0m0.6952   \u001b[0m | \u001b[0m0.8299   \u001b[0m | \u001b[0m0.9309   \u001b[0m | \u001b[0m7.88     \u001b[0m | \u001b[0m28.6     \u001b[0m | \u001b[0m31.94    \u001b[0m |\n",
      "하이퍼 파라미터: {'num_leaves': 37, 'lambda_l1': 0.8069422448218244, 'lambda_l2': 0.9757043485406661, 'feature_fraction': 0.6837697894740693, 'bagging_fraction': 0.7295929036037181, 'min_child_samples': 8, 'min_child_weight': 34.57982411404511, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's binary_logloss: 0.153589\tvalid_0's gini: 0.254807\n",
      "[200]\tvalid_0's binary_logloss: 0.15258\tvalid_0's gini: 0.261213\n",
      "[300]\tvalid_0's binary_logloss: 0.152016\tvalid_0's gini: 0.266309\n",
      "[400]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.270413\n",
      "[500]\tvalid_0's binary_logloss: 0.151468\tvalid_0's gini: 0.273133\n",
      "[600]\tvalid_0's binary_logloss: 0.151335\tvalid_0's gini: 0.275237\n",
      "[700]\tvalid_0's binary_logloss: 0.151247\tvalid_0's gini: 0.276724\n",
      "[800]\tvalid_0's binary_logloss: 0.151179\tvalid_0's gini: 0.278153\n",
      "[900]\tvalid_0's binary_logloss: 0.15112\tvalid_0's gini: 0.279535\n",
      "[1000]\tvalid_0's binary_logloss: 0.151071\tvalid_0's gini: 0.28083\n",
      "[1100]\tvalid_0's binary_logloss: 0.151045\tvalid_0's gini: 0.281631\n",
      "[1200]\tvalid_0's binary_logloss: 0.151023\tvalid_0's gini: 0.282316\n",
      "[1300]\tvalid_0's binary_logloss: 0.150998\tvalid_0's gini: 0.283057\n",
      "[1400]\tvalid_0's binary_logloss: 0.15099\tvalid_0's gini: 0.283326\n",
      "[1500]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.283796\n",
      "[1600]\tvalid_0's binary_logloss: 0.150973\tvalid_0's gini: 0.283959\n",
      "[1700]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.284156\n",
      "[1800]\tvalid_0's binary_logloss: 0.150965\tvalid_0's gini: 0.284309\n",
      "[1900]\tvalid_0's binary_logloss: 0.150955\tvalid_0's gini: 0.284754\n",
      "[2000]\tvalid_0's binary_logloss: 0.150949\tvalid_0's gini: 0.285115\n",
      "[2100]\tvalid_0's binary_logloss: 0.150947\tvalid_0's gini: 0.285264\n",
      "[2200]\tvalid_0's binary_logloss: 0.150948\tvalid_0's gini: 0.28531\n",
      "[2300]\tvalid_0's binary_logloss: 0.150948\tvalid_0's gini: 0.285403\n",
      "Early stopping, best iteration is:\n",
      "[2051]\tvalid_0's binary_logloss: 0.150945\tvalid_0's gini: 0.285297\n",
      "지니계수 : 0.28529747737312666\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.2853   \u001b[0m | \u001b[0m0.7296   \u001b[0m | \u001b[0m0.6838   \u001b[0m | \u001b[0m0.8069   \u001b[0m | \u001b[0m0.9757   \u001b[0m | \u001b[0m8.408    \u001b[0m | \u001b[0m34.58    \u001b[0m | \u001b[0m36.57    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 최적화 수행\n",
    "# 하이퍼 파라미터를 추정하는 과정\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer_xgb = BayesianOptimization(f=eval_function_xgb, # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds_xgb, # 하이퍼파라미터 범위\n",
    "                                 random_state=0)\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "# maximize() 메서드를 호출하여 베이지안 최적화를 수행한다.\n",
    "optimizer_xgb.maximize(init_points=3, n_iter=6)\n",
    "\n",
    "# init_points : 무작위로 하이퍼파라미터를 탐색하는 횟수\n",
    "# n_iter : 베이지안 최적화 반복 횟수\n",
    "# 반복하는 횟수 : init_points + n_iter e.g. 3+6 = 9\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer_lgb = BayesianOptimization(f=eval_function_lgb, # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds_lgb, # 하이퍼파라미터 범위\n",
    "                                 random_state=0)\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "# maximize() 메서드를 호출하여 베이지안 최적화를 수행한다.\n",
    "optimizer_lgb.maximize(init_points=3, n_iter=6)\n",
    "\n",
    "# init_points : 무작위로 하이퍼파라미터를 탐색하는 횟수\n",
    "# n_iter : 베이지안 최적화 반복 횟수\n",
    "# 반복하는 횟수 : init_points + n_iter e.g. 3+6 = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d1a496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.8817801730078565, 'gamma': 9.013424730095146, 'max_depth': 6.927417000715145, 'min_child_weight': 6.992334203641873, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'subsample': 0.6931141936797243}\n",
      "\n",
      "{'colsample_bytree': 0.8817801730078565, 'gamma': 9.013424730095146, 'max_depth': 7, 'min_child_weight': 6.992334203641873, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'subsample': 0.6931141936797243, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 46}\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 결과 확인\n",
    "# 지니계수가 최대가 되는 최적의 하이퍼 파라미터를 얻을 수 있음.\n",
    "\n",
    "# 평가점수(지니계수)가 최대일 때 하이퍼파라미터\n",
    "max_params_xgb = optimizer_xgb.max['params']\n",
    "\n",
    "print(max_params_xgb)\n",
    "print()\n",
    "\n",
    "# max_depth는 트리의 깊이를 의미하는 정수형 하이퍼 파라미터 이므로 정수형으로 변환하여 다시 저장한다.\n",
    "max_params_xgb['max_depth'] = int(round(max_params_xgb['max_depth']))\n",
    "\n",
    "# max_params에 fixed_params를 추가한다. - 고정 파라미터 추가\n",
    "max_params_xgb.update(fixed_params_xgb)\n",
    "\n",
    "# 찐 최종 파라미터 - 그리드 서치와는 달리 베이지안 최적화는 최적 예측기(최적 하이퍼파라미터 값으로 훈련된 모델)를 제공하지 않는다! -> 훈련 필요\n",
    "print(max_params_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d4dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.6213108174593661, 'feature_fraction': 0.608712929970154, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'min_child_samples': 9.112627003799401, 'min_child_weight': 36.10036444740457, 'num_leaves': 39.78618342232764}\n",
      "\n",
      "{'bagging_fraction': 0.6213108174593661, 'feature_fraction': 0.608712929970154, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'num_leaves': 40, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 46}\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 결과 확인\n",
    "# 지니계수가 최대가 되는 최적의 하이퍼 파라미터를 얻을 수 있음.\n",
    "\n",
    "# 평가점수(지니계수)가 최대일 때 하이퍼파라미터\n",
    "max_params_lgb = optimizer_lgb.max['params']\n",
    "\n",
    "print(max_params_lgb)\n",
    "print()\n",
    "\n",
    "# nums_leaves와 min_child_samples는 정수형 하이퍼 파라미터 이므로 정수형으로 변환하여 다시 저장한다.\n",
    "max_params_lgb['num_leaves'] = int(round(max_params_lgb['num_leaves']))\n",
    "max_params_lgb['min_child_samples'] = int(round(max_params_lgb['min_child_samples']))\n",
    "\n",
    "# max_params에 fixed_params를 추가한다. - 고정 파라미터 추가\n",
    "max_params_lgb.update(fixed_params_lgb)\n",
    "\n",
    "# 찐 최종 파라미터 - 그리드 서치와는 달리 베이지안 최적화는 최적 예측기(최적 하이퍼파라미터 값으로 훈련된 모델)를 제공하지 않는다! -> 훈련 필요\n",
    "print(max_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca69309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF 구현\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기\n",
    "# 타깃값이 불균형 하므로, K폴드가 아닌 층화 K 폴드 방식을 사용함.\n",
    "# 층화 K 폴드 방식 : 타깃값이 균등하게 배치되게 폴드를 나누는 방식. -> 하나하나 동일한 비율로 나눠주는 것\n",
    "# 훈련데이터를 섞어줌. 특정 패턴 데이터가 일부 폴드에만 몰려있으면 모델 성능이 과적합 되므로.\n",
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cc51b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. lgb 모델\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타겟값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds_lgb = np.zeros(X.shape[0]) # 훈련 데이터 크기만큼 설정. 폴더로 나눠도 어차피 모두 합하면 K개임.\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타겟값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_lgb = np.zeros(X_test.shape[0]) # 테스트 데이터를 통해 예측한 확률값을 저장함.\n",
    "\n",
    "# 2. xgb 모델\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타겟값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds_xgb = np.zeros(X.shape[0]) # 훈련 데이터 크기만큼 설정. 폴더로 나눠도 어차피 모두 합하면 K개임.\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타겟값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_xgb = np.zeros(X_test.shape[0]) # 테스트 데이터를 통해 예측한 확률값을 저장함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfbc00ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.67668\tvalid-gini:0.15793\n",
      "[100]\tvalid-logloss:0.19139\tvalid-gini:0.25623\n",
      "[200]\tvalid-logloss:0.15803\tvalid-gini:0.28074\n",
      "[300]\tvalid-logloss:0.15476\tvalid-gini:0.28877\n",
      "[400]\tvalid-logloss:0.15422\tvalid-gini:0.29288\n",
      "[500]\tvalid-logloss:0.15407\tvalid-gini:0.29506\n",
      "[600]\tvalid-logloss:0.15401\tvalid-gini:0.29619\n",
      "[700]\tvalid-logloss:0.15396\tvalid-gini:0.29712\n",
      "[800]\tvalid-logloss:0.15393\tvalid-gini:0.29764\n",
      "[900]\tvalid-logloss:0.15394\tvalid-gini:0.29733\n",
      "[1000]\tvalid-logloss:0.15394\tvalid-gini:0.29742\n",
      "[1006]\tvalid-logloss:0.15394\tvalid-gini:0.29746\n",
      "폴드 1 지니계수: 0.2976133308532349\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67668\tvalid-gini:0.15044\n",
      "[100]\tvalid-logloss:0.19145\tvalid-gini:0.23688\n",
      "[200]\tvalid-logloss:0.15821\tvalid-gini:0.26901\n",
      "[300]\tvalid-logloss:0.15497\tvalid-gini:0.28036\n",
      "[400]\tvalid-logloss:0.15447\tvalid-gini:0.28447\n",
      "[500]\tvalid-logloss:0.15436\tvalid-gini:0.28614\n",
      "[600]\tvalid-logloss:0.15431\tvalid-gini:0.28726\n",
      "[700]\tvalid-logloss:0.15430\tvalid-gini:0.28793\n",
      "[800]\tvalid-logloss:0.15430\tvalid-gini:0.28802\n",
      "[900]\tvalid-logloss:0.15429\tvalid-gini:0.28789\n",
      "[1000]\tvalid-logloss:0.15431\tvalid-gini:0.28761\n",
      "[1003]\tvalid-logloss:0.15431\tvalid-gini:0.28760\n",
      "폴드 2 지니계수: 0.28806954596419104\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67668\tvalid-gini:0.14899\n",
      "[100]\tvalid-logloss:0.19148\tvalid-gini:0.24241\n",
      "[200]\tvalid-logloss:0.15820\tvalid-gini:0.26996\n",
      "[300]\tvalid-logloss:0.15504\tvalid-gini:0.27783\n",
      "[400]\tvalid-logloss:0.15455\tvalid-gini:0.28151\n",
      "[500]\tvalid-logloss:0.15439\tvalid-gini:0.28411\n",
      "[600]\tvalid-logloss:0.15434\tvalid-gini:0.28534\n",
      "[700]\tvalid-logloss:0.15431\tvalid-gini:0.28619\n",
      "[800]\tvalid-logloss:0.15432\tvalid-gini:0.28627\n",
      "[900]\tvalid-logloss:0.15431\tvalid-gini:0.28662\n",
      "[1000]\tvalid-logloss:0.15430\tvalid-gini:0.28664\n",
      "[1100]\tvalid-logloss:0.15432\tvalid-gini:0.28644\n",
      "[1182]\tvalid-logloss:0.15432\tvalid-gini:0.28618\n",
      "폴드 3 지니계수: 0.2867111449320017\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67668\tvalid-gini:0.17715\n",
      "[100]\tvalid-logloss:0.19138\tvalid-gini:0.24721\n",
      "[200]\tvalid-logloss:0.15815\tvalid-gini:0.27345\n",
      "[300]\tvalid-logloss:0.15492\tvalid-gini:0.28306\n",
      "[400]\tvalid-logloss:0.15439\tvalid-gini:0.28794\n",
      "[500]\tvalid-logloss:0.15428\tvalid-gini:0.28967\n",
      "[600]\tvalid-logloss:0.15423\tvalid-gini:0.29053\n",
      "[700]\tvalid-logloss:0.15420\tvalid-gini:0.29105\n",
      "[800]\tvalid-logloss:0.15420\tvalid-gini:0.29123\n",
      "[900]\tvalid-logloss:0.15418\tvalid-gini:0.29105\n",
      "[1000]\tvalid-logloss:0.15420\tvalid-gini:0.29096\n",
      "[1008]\tvalid-logloss:0.15421\tvalid-gini:0.29103\n",
      "폴드 4 지니계수: 0.291245878357585\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67668\tvalid-gini:0.16568\n",
      "[100]\tvalid-logloss:0.19150\tvalid-gini:0.23613\n",
      "[200]\tvalid-logloss:0.15843\tvalid-gini:0.25991\n",
      "[300]\tvalid-logloss:0.15536\tvalid-gini:0.26947\n",
      "[400]\tvalid-logloss:0.15489\tvalid-gini:0.27346\n",
      "[500]\tvalid-logloss:0.15477\tvalid-gini:0.27599\n",
      "[600]\tvalid-logloss:0.15474\tvalid-gini:0.27687\n",
      "[700]\tvalid-logloss:0.15474\tvalid-gini:0.27755\n",
      "[800]\tvalid-logloss:0.15473\tvalid-gini:0.27790\n",
      "[900]\tvalid-logloss:0.15472\tvalid-gini:0.27798\n",
      "[1000]\tvalid-logloss:0.15472\tvalid-gini:0.27793\n",
      "[1100]\tvalid-logloss:0.15472\tvalid-gini:0.27848\n",
      "[1200]\tvalid-logloss:0.15471\tvalid-gini:0.27869\n",
      "[1300]\tvalid-logloss:0.15471\tvalid-gini:0.27854\n",
      "[1383]\tvalid-logloss:0.15472\tvalid-gini:0.27834\n",
      "폴드 5 지니계수: 0.2787341361820848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoot 모델 훈련 + OOF 예측\n",
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델을 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    #folds.split : 데이터를 k개로 나눔\n",
    "    \n",
    "    #각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx] #훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] #검증용 데이터\n",
    "    \n",
    "    # XGBoost 전용 데이터셋 생성 \n",
    "    dtrain = xgb.DMatrix(X_train, y_train) # XGBoost 전용 훈련 데이터 셋\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid) # XGBoost 전용 검증 데이터 셋\n",
    "    dtest = xgb.DMatrix(X_test) # XGBoost 전용 테스트 테스트 셋\n",
    "    \n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=max_params_xgb, #최적 하이퍼 파라미터로 변경\n",
    "                          dtrain=dtrain, #훈련 데이터 셋\n",
    "                          num_boost_round=2000, #부스팅 반복 횟수\n",
    "                          # 클수록 성능이 좋아히지나 과대적합 우려.\n",
    "                          # 작을수록 반복횟수가 줄어들어 훈련시간이 짧아짐.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야함.\n",
    "                          evals=[(dvalid, 'valid')], #성능 평가용 검증 데이터 셋\n",
    "                          maximize=True,# gini로 평가할 때 평가점수가 높으면 좋은지 여부 - True\n",
    "                          feval=gini_xgb, #검증용 평가지표\n",
    "                          early_stopping_rounds=200, #조기종료 조건\n",
    "                          # num_boost_round 만큼 훈련을 반복하는데, 매 이터레이션 마다 eval로 평가시 성능이 연속적으로 좋아지지않으면 중단.\n",
    "                          verbose_eval=100) # 100번째마다 점수 출력 - 출력값이 많아지는 걸 방지.\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter = xgb_model.best_iteration\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측 - folds.n_splits로 나눈 이유는 확률을 구해야하므로.\n",
    "     # predict() 호출시 훈련 단계에서 최고 성능을 낸 반복 횟수로 예측한다.\n",
    "    # e.g. 500번째 이터레이션에서 지니계수가 최고라면 500번째꺼 사용함.(default)\n",
    "    oof_test_preds_xgb += xgb_model.predict(dtest,\n",
    "                                        iteration_range=(0, best_iter))/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타겟값 예측\n",
    "    # valid_idx에 해당하는 값만 검증 데이터 예측 확률로 업데이트해줌.\n",
    "    # 폴드가 5번 반복되면 off_val_preds 내 모든 값이 검증 데이터 예측확률로 업데이트됨.\n",
    "    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, \n",
    "                                                  iteration_range=(0, best_iter))\n",
    "    \n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    #\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfa27771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154255\tvalid_0's gini: 0.273723\n",
      "[200]\tvalid_0's binary_logloss: 0.15317\tvalid_0's gini: 0.279059\n",
      "[300]\tvalid_0's binary_logloss: 0.152573\tvalid_0's gini: 0.28264\n",
      "[400]\tvalid_0's binary_logloss: 0.152213\tvalid_0's gini: 0.285212\n",
      "[500]\tvalid_0's binary_logloss: 0.151973\tvalid_0's gini: 0.28789\n",
      "[600]\tvalid_0's binary_logloss: 0.151807\tvalid_0's gini: 0.289982\n",
      "[700]\tvalid_0's binary_logloss: 0.151676\tvalid_0's gini: 0.292139\n",
      "[800]\tvalid_0's binary_logloss: 0.15158\tvalid_0's gini: 0.293764\n",
      "[900]\tvalid_0's binary_logloss: 0.151521\tvalid_0's gini: 0.294574\n",
      "[1000]\tvalid_0's binary_logloss: 0.151468\tvalid_0's gini: 0.295428\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's binary_logloss: 0.151468\tvalid_0's gini: 0.295449\n",
      "폴드 1 지니계수: 0.2954486602083161\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154386\tvalid_0's gini: 0.258187\n",
      "[200]\tvalid_0's binary_logloss: 0.15337\tvalid_0's gini: 0.262656\n",
      "[300]\tvalid_0's binary_logloss: 0.152802\tvalid_0's gini: 0.268435\n",
      "[400]\tvalid_0's binary_logloss: 0.15245\tvalid_0's gini: 0.27284\n",
      "[500]\tvalid_0's binary_logloss: 0.152235\tvalid_0's gini: 0.276004\n",
      "[600]\tvalid_0's binary_logloss: 0.15209\tvalid_0's gini: 0.278502\n",
      "[700]\tvalid_0's binary_logloss: 0.151992\tvalid_0's gini: 0.280199\n",
      "[800]\tvalid_0's binary_logloss: 0.151916\tvalid_0's gini: 0.281855\n",
      "[900]\tvalid_0's binary_logloss: 0.151866\tvalid_0's gini: 0.282971\n",
      "[1000]\tvalid_0's binary_logloss: 0.151825\tvalid_0's gini: 0.283888\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151825\tvalid_0's gini: 0.283888\n",
      "폴드 2 지니계수: 0.28388826556017244\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154244\tvalid_0's gini: 0.262744\n",
      "[200]\tvalid_0's binary_logloss: 0.153213\tvalid_0's gini: 0.267641\n",
      "[300]\tvalid_0's binary_logloss: 0.152658\tvalid_0's gini: 0.27131\n",
      "[400]\tvalid_0's binary_logloss: 0.152326\tvalid_0's gini: 0.274369\n",
      "[500]\tvalid_0's binary_logloss: 0.152119\tvalid_0's gini: 0.276832\n",
      "[600]\tvalid_0's binary_logloss: 0.151974\tvalid_0's gini: 0.279099\n",
      "[700]\tvalid_0's binary_logloss: 0.151859\tvalid_0's gini: 0.281487\n",
      "[800]\tvalid_0's binary_logloss: 0.151783\tvalid_0's gini: 0.283005\n",
      "[900]\tvalid_0's binary_logloss: 0.151723\tvalid_0's gini: 0.284377\n",
      "[1000]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.285374\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.285374\n",
      "폴드 3 지니계수: 0.2853743869360686\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1549\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154249\tvalid_0's gini: 0.266956\n",
      "[200]\tvalid_0's binary_logloss: 0.153195\tvalid_0's gini: 0.27198\n",
      "[300]\tvalid_0's binary_logloss: 0.152622\tvalid_0's gini: 0.27631\n",
      "[400]\tvalid_0's binary_logloss: 0.152278\tvalid_0's gini: 0.279244\n",
      "[500]\tvalid_0's binary_logloss: 0.152046\tvalid_0's gini: 0.282313\n",
      "[600]\tvalid_0's binary_logloss: 0.15189\tvalid_0's gini: 0.284887\n",
      "[700]\tvalid_0's binary_logloss: 0.151785\tvalid_0's gini: 0.28663\n",
      "[800]\tvalid_0's binary_logloss: 0.151705\tvalid_0's gini: 0.288167\n",
      "[900]\tvalid_0's binary_logloss: 0.151646\tvalid_0's gini: 0.28951\n",
      "[1000]\tvalid_0's binary_logloss: 0.151603\tvalid_0's gini: 0.290424\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151603\tvalid_0's gini: 0.290424\n",
      "폴드 4 지니계수: 0.2904239554579626\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154433\tvalid_0's gini: 0.249436\n",
      "[200]\tvalid_0's binary_logloss: 0.153474\tvalid_0's gini: 0.256173\n",
      "[300]\tvalid_0's binary_logloss: 0.15296\tvalid_0's gini: 0.26115\n",
      "[400]\tvalid_0's binary_logloss: 0.152677\tvalid_0's gini: 0.264329\n",
      "[500]\tvalid_0's binary_logloss: 0.152504\tvalid_0's gini: 0.267089\n",
      "[600]\tvalid_0's binary_logloss: 0.152382\tvalid_0's gini: 0.269714\n",
      "[700]\tvalid_0's binary_logloss: 0.152304\tvalid_0's gini: 0.271627\n",
      "[800]\tvalid_0's binary_logloss: 0.152247\tvalid_0's gini: 0.273155\n",
      "[900]\tvalid_0's binary_logloss: 0.152208\tvalid_0's gini: 0.274375\n",
      "[1000]\tvalid_0's binary_logloss: 0.152174\tvalid_0's gini: 0.275493\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.152174\tvalid_0's gini: 0.275493\n",
      "폴드 5 지니계수: 0.2754934692434143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LightGBM 모델 훈련 + OOF 예측\n",
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델을 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    #folds.split : 데이터를 k개로 나눔\n",
    "    \n",
    "    #각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx] #훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] #검증용 데이터\n",
    "    \n",
    "    # LightGM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터 셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터 셋\n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=max_params_lgb, #훈련용 하이퍼 파라미터 -> 최적 하이퍼 파라미터로 변경\n",
    "                          train_set=dtrain, #훈련 데이터 셋\n",
    "                          num_boost_round=1000, #부스팅 반복 횟수\n",
    "                          # 클수록 성능이 좋아히지나 과대적합 우려.\n",
    "                          # 작을수록 반복횟수가 줄어들어 훈련시간이 짧아짐.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야함.\n",
    "                          valid_sets=dvalid, #성능 평가용 검증 데이터 셋\n",
    "                          feval=gini_lgb, #검증용 평가지표\n",
    "                          early_stopping_rounds=100, #조기종료 조건\n",
    "                          # num_boost_round 만큼 훈련을 반복하는데, 매 이터레이션 마다 eval로 평가시 성능이 연속적으로 좋아지지않으면 중단.\n",
    "                          verbose_eval=100) # 100번째마다 점수 출력 - 출력값이 많아지는 걸 방지.\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측 - folds.n_splits로 나눈 이유는 확률을 구해야하므로.\n",
    "     # predict() 호출시 훈련 단계에서 최고 성능을 낸 반복 횟수로 예측한다.\n",
    "    # e.g. 500번째 이터레이션에서 지니계수가 최고라면 500번째꺼 사용함.(default)\n",
    "    # 주의할점은 X_test, X-valid 그대로 사용해야한다는 점 lgb.Dataset()변환 X\n",
    "    oof_test_preds_lgb += lgb_model.predict(X_test)/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타겟값 예측\n",
    "    # valid_idx에 해당하는 값만 검증 데이터 예측 확률로 업데이트해줌.\n",
    "    # 폴드가 5번 반복되면 off_val_preds 내 모든 값이 검증 데이터 예측확률로 업데이트됨.\n",
    "    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    #\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수: {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9095975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost : OOF 검증 데이터 지니계수 :  0.2883263608025937\n",
      "LightGBM : OOF 검증 데이터 지니계수 :  0.286007908703013\n"
     ]
    }
   ],
   "source": [
    "# 훈련 종료. 결과.\n",
    "# 검증 데이터로 예측한 확률을 실제 타깃값과 비교하여 지니계수를 출력함.\n",
    "print('XGBoost : OOF 검증 데이터 지니계수 : ', eval_gini(y, oof_val_preds_xgb))\n",
    "print('LightGBM : OOF 검증 데이터 지니계수 : ', eval_gini(y, oof_val_preds_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5269c38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2884102499226329"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_val_preds = oof_val_preds_xgb * 0.5 + oof_val_preds_lgb * 0.5\n",
    "eval_gini(y, off_val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c304d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 및 결과 제출\n",
    "submission['target'] = oof_test_preds_xgb * 0.5 + oof_test_preds_lgb * 0.5\n",
    "# 여기가 앙상블 과정 : 각각의 모델에 50%의 가중치를 주어 구한 가중평균을 이용함.\n",
    "\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe82725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
