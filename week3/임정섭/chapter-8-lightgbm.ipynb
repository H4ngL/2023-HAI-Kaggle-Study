{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-04-06T09:30:40.067013Z",
     "iopub.execute_input": "2023-04-06T09:30:40.067506Z",
     "iopub.status.idle": "2023-04-06T09:30:40.083380Z",
     "shell.execute_reply.started": "2023-04-06T09:30:40.067467Z",
     "shell.execute_reply": "2023-04-06T09:30:40.081405Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/porto-seguro-safe-driver-prediction/sample_submission.csv\n/kaggle/input/porto-seguro-safe-driver-prediction/train.csv\n/kaggle/input/porto-seguro-safe-driver-prediction/test.csv\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 데이터 경로\ndata_path = '/kaggle/input/porto-seguro-safe-driver-prediction/'\n\ntrain = pd.read_csv(data_path + 'train.csv', index_col='id')\ntest = pd.read_csv(data_path + 'test.csv', index_col='id')\nsubmission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:30:40.429569Z",
     "iopub.execute_input": "2023-04-06T09:30:40.430068Z",
     "iopub.status.idle": "2023-04-06T09:30:53.666281Z",
     "shell.execute_reply.started": "2023-04-06T09:30:40.430024Z",
     "shell.execute_reply": "2023-04-06T09:30:53.665245Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 피처 엔저니어링",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 데이터 전처리 - 앞선 노트북에서 다룬 내용",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "all_data = pd.concat([train, test], ignore_index=True)\nall_data = all_data.drop('target', axis=1) # 타깃값 제거\n\nall_features = all_data.columns # 전체 피처\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 명목형 피처\ncat_features = [feature for feature in all_features if 'cat' in feature] \n\n# 원-핫 인코딩 적용\nonehot_encoder = OneHotEncoder()\nencoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) \n\n# '데이터 하나당 결측값 개수'를 파생 피처로 추가\nall_data['num_missing'] = (all_data==-1).sum(axis=1)\n\n# 명목형 피처, calc 분류의 피처를 제외한 피처\nremaining_features = [feature for feature in all_features\n                      if ('cat' not in feature and 'calc' not in feature)] \n# num_missing을 remaining_features에 추가\nremaining_features.append('num_missing')\n# 분류가 ind인 피처\nind_features = [feature for feature in all_features if 'ind' in feature]\n\nis_first_feature = True\nfor ind_feature in ind_features:\n    if is_first_feature:\n        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n        is_first_feature = False\n    else:\n        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'\n        \nall_data['mix_ind']\n\ncat_count_features = []\nfor feature in cat_features+['mix_ind']:\n    val_counts_dict = all_data[feature].value_counts().to_dict()\n    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: \n                                                           val_counts_dict[x])\n    cat_count_features.append(f'{feature}_count')\ncat_count_features",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:34:41.571116Z",
     "iopub.execute_input": "2023-04-06T09:34:41.572400Z",
     "iopub.status.idle": "2023-04-06T09:35:21.598011Z",
     "shell.execute_reply.started": "2023-04-06T09:34:41.572351Z",
     "shell.execute_reply": "2023-04-06T09:35:21.596526Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ps_ind_02_cat_count',\n 'ps_ind_04_cat_count',\n 'ps_ind_05_cat_count',\n 'ps_car_01_cat_count',\n 'ps_car_02_cat_count',\n 'ps_car_03_cat_count',\n 'ps_car_04_cat_count',\n 'ps_car_05_cat_count',\n 'ps_car_06_cat_count',\n 'ps_car_07_cat_count',\n 'ps_car_08_cat_count',\n 'ps_car_09_cat_count',\n 'ps_car_10_cat_count',\n 'ps_car_11_cat_count',\n 'mix_ind_count']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from scipy import sparse\n# 필요 없는 피처들\ndrop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', \n                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n\n# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\nall_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n\n# 데이터 합치기\nall_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n                               encoded_cat_matrix],\n                              format='csr')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:35:21.600517Z",
     "iopub.execute_input": "2023-04-06T09:35:21.601976Z",
     "iopub.status.idle": "2023-04-06T09:35:30.158778Z",
     "shell.execute_reply.started": "2023-04-06T09:35:21.601912Z",
     "shell.execute_reply": "2023-04-06T09:35:30.157445Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "num_train = len(train) # 훈련 데이터 개수\n\n# 훈련 데이터와 테스트 데이터 나누기\nX = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\n\ny = train['target'].values",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:35:30.160765Z",
     "iopub.execute_input": "2023-04-06T09:35:30.161607Z",
     "iopub.status.idle": "2023-04-06T09:35:31.725596Z",
     "shell.execute_reply.started": "2023-04-06T09:35:30.161553Z",
     "shell.execute_reply": "2023-04-06T09:35:31.724136Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\ndef eval_gini(y_true, y_pred):\n    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)\n    assert y_true.shape == y_pred.shape\n\n    n_samples = y_true.shape[0]                      # 데이터 개수\n    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n\n    # 1) 예측값에 대한 지니계수\n    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n\n    # 2) 예측이 완벽할 때 지니계수\n    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n\n    # 정규화된 지니계수\n    return G_pred / G_true",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:35:31.728871Z",
     "iopub.execute_input": "2023-04-06T09:35:31.729291Z",
     "iopub.status.idle": "2023-04-06T09:35:31.738485Z",
     "shell.execute_reply.started": "2023-04-06T09:35:31.729256Z",
     "shell.execute_reply": "2023-04-06T09:35:31.736950Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# LightGBM용 gini() 함수\ndef gini(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', eval_gini(labels, preds), True # 반환값",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:35:31.739956Z",
     "iopub.execute_input": "2023-04-06T09:35:31.740325Z",
     "iopub.status.idle": "2023-04-06T09:35:31.750347Z",
     "shell.execute_reply.started": "2023-04-06T09:35:31.740291Z",
     "shell.execute_reply": "2023-04-06T09:35:31.748990Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 하이퍼 파라미터 최적화 (베이지안 최적화)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 데이터셋 준비",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\n# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size=0.2, \n                                                      random_state=0)\n\n# 베이지안 최적화용 데이터셋\nbayes_dtrain = lgb.Dataset(X_train, y_train)\nbayes_dvalid = lgb.Dataset(X_valid, y_valid)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:35:46.589125Z",
     "iopub.execute_input": "2023-04-06T09:35:46.589586Z",
     "iopub.status.idle": "2023-04-06T09:35:49.186585Z",
     "shell.execute_reply.started": "2023-04-06T09:35:46.589549Z",
     "shell.execute_reply": "2023-04-06T09:35:49.185352Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 하이퍼파라미터 범위 설정",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 베이지안 최적화를 위한 하이퍼파라미터 범위\nparam_bounds = {'num_leaves': (30, 40),\n                'lambda_l1': (0.7, 0.9),\n                'lambda_l2': (0.9, 1),\n                'feature_fraction': (0.6, 0.7),\n                'bagging_fraction': (0.6, 0.9),\n                'min_child_samples': (6, 10),\n                'min_child_weight': (10, 40)}\n\n# 값이 고정된 하이퍼파라미터\nfixed_params = {'objective': 'binary',\n                'learning_rate': 0.005,\n                'bagging_freq': 1,\n                'force_row_wise': True,\n                'random_state': 1991}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:36:07.913598Z",
     "iopub.execute_input": "2023-04-06T09:36:07.915174Z",
     "iopub.status.idle": "2023-04-06T09:36:07.926531Z",
     "shell.execute_reply.started": "2023-04-06T09:36:07.915099Z",
     "shell.execute_reply": "2023-04-06T09:36:07.924584Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n                  bagging_fraction, min_child_samples, min_child_weight):\n    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n    \n    # 베이지안 최적화를 수행할 하이퍼파라미터 \n    params = {'num_leaves': int(round(num_leaves)),\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_child_samples': int(round(min_child_samples)),\n              'min_child_weight': min_child_weight,\n              'feature_pre_filter': False}\n    # 고정된 하이퍼파라미터도 추가\n    params.update(fixed_params)\n    \n    print('하이퍼파라미터:', params)    \n    \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=params, \n                           train_set=bayes_dtrain,\n                           num_boost_round=2500,\n                           valid_sets=bayes_dvalid,\n                           feval=gini,\n                           early_stopping_rounds=300,\n                           verbose_eval=False)\n    # 검증 데이터로 예측 수행\n    preds = lgb_model.predict(X_valid) \n    # 지니계수 계산\n    gini_score = eval_gini(y_valid, preds)\n    print(f'지니계수 : {gini_score}\\n')\n    \n    return gini_score",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:36:34.137875Z",
     "iopub.execute_input": "2023-04-06T09:36:34.138373Z",
     "iopub.status.idle": "2023-04-06T09:36:34.149098Z",
     "shell.execute_reply.started": "2023-04-06T09:36:34.138330Z",
     "shell.execute_reply": "2023-04-06T09:36:34.147639Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 최적화 수행",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from bayes_opt import BayesianOptimization\n\n# 베이지안 최적화 객체 생성\noptimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n                                 random_state=0)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:36:49.692950Z",
     "iopub.execute_input": "2023-04-06T09:36:49.693472Z",
     "iopub.status.idle": "2023-04-06T09:36:49.746476Z",
     "shell.execute_reply.started": "2023-04-06T09:36:49.693426Z",
     "shell.execute_reply": "2023-04-06T09:36:49.745387Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 베이지안 최적화 수행\noptimizer.maximize(init_points=3, n_iter=6)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-06T09:36:56.364406Z",
     "iopub.execute_input": "2023-04-06T09:36:56.364930Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n-------------------------------------------------------------------------------------------------------------\n하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n[LightGBM] [Info] Total Bins 1555\n[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n[LightGBM] [Info] Start training from score -3.273091\n지니계수 : 0.2855811556220905\n\n| \u001B[0m1        \u001B[0m | \u001B[0m0.2856   \u001B[0m | \u001B[0m0.7646   \u001B[0m | \u001B[0m0.6715   \u001B[0m | \u001B[0m0.8206   \u001B[0m | \u001B[0m0.9545   \u001B[0m | \u001B[0m7.695    \u001B[0m | \u001B[0m29.38    \u001B[0m | \u001B[0m34.38    \u001B[0m |\n하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n[LightGBM] [Info] Total Bins 1555\n[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n[LightGBM] [Info] Start training from score -3.273091\n지니계수 : 0.2837380537005777\n\n| \u001B[0m2        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8675   \u001B[0m | \u001B[0m0.6964   \u001B[0m | \u001B[0m0.7767   \u001B[0m | \u001B[0m0.9792   \u001B[0m | \u001B[0m8.116    \u001B[0m | \u001B[0m27.04    \u001B[0m | \u001B[0m39.26    \u001B[0m |\n하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n[LightGBM] [Info] Total Bins 1555\n[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n[LightGBM] [Info] Start training from score -3.273091\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 결과",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 평가함수 점수가 최대일 때 하이퍼파라미터\nmax_params = optimizer.max['params']\nmax_params",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 정수형 하이퍼파라미터 변환\nmax_params['num_leaves'] = int(round(max_params['num_leaves']))\nmax_params['min_child_samples'] = int(round(max_params['min_child_samples']))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 값이 고정된 하이퍼파라미터 추가\nmax_params.update(fixed_params)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "max_params",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 모델 훈련 및 성능 검증",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import StratifiedKFold\n\n# 층화 K 폴드 교차 검증기 생성\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n\n# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_val_preds = np.zeros(X.shape[0]) \n# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\noof_test_preds = np.zeros(X_test.shape[0]) \n\n# OOF 방식으로 모델 훈련, 검증, 예측\nfor idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n    # 각 폴드를 구분하는 문구 출력\n    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n    \n    # 훈련용 데이터, 검증용 데이터 설정\n    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n\n    # LightGBM 전용 데이터셋 생성\n    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n                          \n    # LightGBM 모델 훈련\n    lgb_model = lgb.train(params=max_params,    # 최적 하이퍼파라미터\n                          train_set=dtrain,     # 훈련 데이터셋\n                          num_boost_round=2500, # 부스팅 반복 횟수\n                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n                          feval=gini,           # 검증용 평가지표\n                          early_stopping_rounds=300, # 조기종료 조건\n                          verbose_eval=100)     # 100번째마다 점수 출력\n    \n    # 테스트 데이터를 활용해 OOF 예측\n    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n    \n    # 검증 데이터 예측확률에 대한 정규화 지니계수\n    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print('OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "submission['target'] = oof_test_preds\nsubmission.to_csv('submission.csv')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
